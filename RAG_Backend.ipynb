{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##**Building a Retrieval Augmented Generation (RAG) Chatbot**\n",
        "\n",
        "Using Gemini, LangChain, and ChromaDB\n",
        "\n",
        "This notebook will guide you through implementing the backend components of a RAG chatbot system."
      ],
      "metadata": {
        "id": "2JVJOcX1taGd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setup and Prerequisites\n",
        "\n",
        "First, let's install the necessary libraries."
      ],
      "metadata": {
        "id": "7ZqSCfqst11s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zFezRBzosgdT",
        "outputId": "84539bc7-9534-4d25-dbfa-55cf159aa816"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.9/323.9 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.8/20.8 MB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.3/132.3 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.9/65.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.0/208.0 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m517.7/517.7 kB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m877.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.4/128.4 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m84.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m456.8/456.8 kB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\n",
            "google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.9.0 which is incompatible.\n",
            "google-adk 1.17.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.38.0 which is incompatible.\n",
            "google-adk 1.17.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.9/67.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m73.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m65.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain-google-genai 2.1.12 requires google-ai-generativelanguage<1,>=0.7, but you have google-ai-generativelanguage 0.6.15 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install langchain langchain-google-genai langchain_community pypdf chromadb sentence-transformers -q\n",
        "!pip install google-generativeai pdfplumber -q"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, let's import all required libraries:"
      ],
      "metadata": {
        "id": "nHh5BTWmuB6d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pdfplumber\n",
        "import google.generativeai as genai\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.memory import ConversationBufferMemory"
      ],
      "metadata": {
        "id": "NKPhaAVvt7jk"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get(\"GEMINI_API_KEY\")"
      ],
      "metadata": {
        "id": "JEN3Zs1guLf_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Section 1: Uploading PDF**\n",
        "In this section, we'll implement the functionality to upload PDF files. For this notebook demonstration, we'll assume the PDF is in a local path."
      ],
      "metadata": {
        "id": "B3jNmp3Suq9H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def upload_pdf(pdf_path):\n",
        "    \"\"\"\n",
        "    Function to handle PDF uploads.\n",
        "\n",
        "    Args:\n",
        "        pdf_path (str): Path to the PDF file\n",
        "\n",
        "    Returns:\n",
        "        str: PDF file path if successful\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # In a real application with Streamlit, you would use:\n",
        "        # uploaded_file = st.file_uploader(\"Choose a PDF file\", type=\"pdf\")\n",
        "        # But for this notebook, we'll just verify the file exists\n",
        "\n",
        "        if os.path.exists(pdf_path):\n",
        "            print(f\"PDF file found at: {pdf_path}\")\n",
        "            return pdf_path\n",
        "        else:\n",
        "            print(f\"Error: File not found at {pdf_path}\")\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error uploading PDF: {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "DVUSC7eMujYA"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attention_paper_path = \"/content/attention_is_all_u_need.pdf\""
      ],
      "metadata": {
        "id": "Mnbe0m7vv1tc"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "upload_pdf(attention_paper_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "xgFzTsM5vXYm",
        "outputId": "4ca1b725-2f66-4f27-f5ae-dee5a94e6f88"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PDF file found at: /content/attention_is_all_u_need.pdf\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/attention_is_all_u_need.pdf'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Section 2: Parsing the PDF and Creating Text Files**\n",
        "Now we'll extract the text content from the uploaded PDFs."
      ],
      "metadata": {
        "id": "TrLtDfvGvrvn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_pdf(pdf_path):\n",
        "    \"\"\"\n",
        "    Function to extract text from PDF files.\n",
        "\n",
        "    Args:\n",
        "        pdf_path (str): Path to the PDF file\n",
        "\n",
        "    Returns:\n",
        "        str: Extracted text from the PDF\n",
        "    \"\"\"\n",
        "    try:\n",
        "        text = \"\"\n",
        "\n",
        "        # Using pdfplumber to extract text\n",
        "        with pdfplumber.open(pdf_path) as pdf:\n",
        "            for page in pdf.pages:\n",
        "                text += page.extract_text() + \"\\n\"\n",
        "\n",
        "        # Save the extracted text to a file (optional)\n",
        "        text_file_path = pdf_path.replace('.pdf', '.txt')\n",
        "        with open(text_file_path, 'w', encoding='utf-8') as f:\n",
        "            f.write(text)\n",
        "\n",
        "        print(f\"PDF parsed successfully, extracted {len(text)} characters\")\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing PDF: {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "6HekDExEvusX"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_file = parse_pdf(attention_paper_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_rhnU_Lv8g7",
        "outputId": "20f186ad-0ca5-4eab-896c-7946c9a69708"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PDF parsed successfully, extracted 35526 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Section 3: Creating Document Chunks**\n",
        "To effectively process and retrieve information, we need to break down our document into manageable chunks."
      ],
      "metadata": {
        "id": "qs_s0duPwIli"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_document_chunks(text):\n",
        "    \"\"\"\n",
        "    Function to split the document text into smaller chunks for processing.\n",
        "\n",
        "    Args:\n",
        "        text (str): The full text from the PDF\n",
        "\n",
        "    Returns:\n",
        "        list: List of text chunks\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Initialize the text splitter\n",
        "        # We can tune these parameters based on our needs and model constraints\n",
        "        text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=500,        # Size of each chunk in characters\n",
        "            chunk_overlap=100,      # Overlap between chunks to maintain context\n",
        "            length_function=len,\n",
        "            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]  # Hierarchy of separators to use when splitting\n",
        "        )\n",
        "\n",
        "        # Split the text into chunks\n",
        "        chunks = text_splitter.split_text(text)\n",
        "\n",
        "        print(f\"Document split into {len(chunks)} chunks\")\n",
        "        print(\"chunks: \", chunks)\n",
        "        return chunks\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating document chunks: {e}\")\n",
        "        return []"
      ],
      "metadata": {
        "id": "1H7JqyQDwEus"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jqM0J_GDeNZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_chunks = create_document_chunks(text_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7d5JC_DZw0y3",
        "outputId": "372114e7-7ef3-4cb8-850e-d05c9460455a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document split into 91 chunks\n",
            "chunks:  ['Providedproperattributionisprovided,Googleherebygrantspermissionto\\nreproducethetablesandfiguresinthispapersolelyforuseinjournalisticor\\nscholarlyworks.\\nAttention Is All You Need\\nAshishVaswani∗ NoamShazeer∗ NikiParmar∗ JakobUszkoreit∗\\nGoogleBrain GoogleBrain GoogleResearch GoogleResearch\\navaswani@google.com noam@google.com nikip@google.com usz@google.com\\nLlionJones∗ AidanN.Gomez∗ † ŁukaszKaiser∗\\nGoogleResearch UniversityofToronto GoogleBrain', 'LlionJones∗ AidanN.Gomez∗ † ŁukaszKaiser∗\\nGoogleResearch UniversityofToronto GoogleBrain\\nllion@google.com aidan@cs.toronto.edu lukaszkaiser@google.com\\nIlliaPolosukhin∗ ‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThedominantsequencetransductionmodelsarebasedoncomplexrecurrentor\\nconvolutionalneuralnetworksthatincludeanencoderandadecoder. Thebest\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,', 'mechanism. We propose a new simple network architecture, the Transformer,\\nbasedsolelyonattentionmechanisms,dispensingwithrecurrenceandconvolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbesuperiorinqualitywhilebeingmoreparallelizableandrequiringsignificantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including', 'to-German translation task, improving over the existing best results, including\\nensembles,byover2BLEU.OntheWMT2014English-to-Frenchtranslationtask,\\nourmodelestablishesanewsingle-modelstate-of-the-artBLEUscoreof41.8after\\ntrainingfor3.5daysoneightGPUs,asmallfractionofthetrainingcostsofthe\\nbestmodelsfromtheliterature. WeshowthattheTransformergeneralizeswellto\\nothertasksbyapplyingitsuccessfullytoEnglishconstituencyparsingbothwith\\nlargeandlimitedtrainingdata.', 'othertasksbyapplyingitsuccessfullytoEnglishconstituencyparsingbothwith\\nlargeandlimitedtrainingdata.\\n∗Equalcontribution.Listingorderisrandom.JakobproposedreplacingRNNswithself-attentionandstarted\\ntheefforttoevaluatethisidea.Ashish,withIllia,designedandimplementedthefirstTransformermodelsand\\nhasbeencruciallyinvolvedineveryaspectofthiswork.Noamproposedscaleddot-productattention,multi-head\\nattentionandtheparameter-freepositionrepresentationandbecametheotherpersoninvolvedinnearlyevery', 'attentionandtheparameter-freepositionrepresentationandbecametheotherpersoninvolvedinnearlyevery\\ndetail.Nikidesigned,implemented,tunedandevaluatedcountlessmodelvariantsinouroriginalcodebaseand\\ntensor2tensor.Llionalsoexperimentedwithnovelmodelvariants,wasresponsibleforourinitialcodebase,and\\nefficientinferenceandvisualizations.LukaszandAidanspentcountlesslongdaysdesigningvariouspartsofand\\nimplementingtensor2tensor,replacingourearliercodebase,greatlyimprovingresultsandmassivelyaccelerating', 'ourresearch.\\n†WorkperformedwhileatGoogleBrain.\\n‡WorkperformedwhileatGoogleResearch.\\n31stConferenceonNeuralInformationProcessingSystems(NIPS2017),LongBeach,CA,USA.\\n3202\\nguA\\n2\\n]LC.sc[\\n7v26730.6071:viXra\\n1 Introduction\\nRecurrentneuralnetworks,longshort-termmemory[13]andgatedrecurrent[7]neuralnetworks\\ninparticular,havebeenfirmlyestablishedasstateoftheartapproachesinsequencemodelingand\\ntransductionproblemssuchaslanguagemodelingandmachinetranslation[35,2,5]. Numerous', 'transductionproblemssuchaslanguagemodelingandmachinetranslation[35,2,5]. Numerous\\neffortshavesincecontinuedtopushtheboundariesofrecurrentlanguagemodelsandencoder-decoder\\narchitectures[38,24,15].\\nRecurrentmodelstypicallyfactorcomputationalongthesymbolpositionsoftheinputandoutput\\nsequences. Aligningthepositionstostepsincomputationtime,theygenerateasequenceofhidden\\nstatesh ,asafunctionoftheprevioushiddenstateh andtheinputforpositiont. Thisinherently\\nt t−1', 'statesh ,asafunctionoftheprevioushiddenstateh andtheinputforpositiont. Thisinherently\\nt t−1\\nsequentialnatureprecludesparallelizationwithintrainingexamples,whichbecomescriticalatlonger\\nsequencelengths,asmemoryconstraintslimitbatchingacrossexamples. Recentworkhasachieved\\nsignificantimprovementsincomputationalefficiencythroughfactorizationtricks[21]andconditional\\ncomputation[32],whilealsoimprovingmodelperformanceincaseofthelatter. Thefundamental\\nconstraintofsequentialcomputation,however,remains.', 'constraintofsequentialcomputation,however,remains.\\nAttentionmechanismshavebecomeanintegralpartofcompellingsequencemodelingandtransduc-\\ntionmodelsinvarioustasks,allowingmodelingofdependencieswithoutregardtotheirdistancein\\ntheinputoroutputsequences[2,19]. Inallbutafewcases[27],however,suchattentionmechanisms\\nareusedinconjunctionwitharecurrentnetwork.\\nInthisworkweproposetheTransformer,amodelarchitectureeschewingrecurrenceandinstead', 'InthisworkweproposetheTransformer,amodelarchitectureeschewingrecurrenceandinstead\\nrelyingentirelyonanattentionmechanismtodrawglobaldependenciesbetweeninputandoutput.\\nTheTransformerallowsforsignificantlymoreparallelizationandcanreachanewstateoftheartin\\ntranslationqualityafterbeingtrainedforaslittleastwelvehoursoneightP100GPUs.\\n2 Background\\nThegoalofreducingsequentialcomputationalsoformsthefoundationoftheExtendedNeuralGPU', '2 Background\\nThegoalofreducingsequentialcomputationalsoformsthefoundationoftheExtendedNeuralGPU\\n[16],ByteNet[18]andConvS2S[9],allofwhichuseconvolutionalneuralnetworksasbasicbuilding\\nblock,computinghiddenrepresentationsinparallelforallinputandoutputpositions.Inthesemodels,\\nthenumberofoperationsrequiredtorelatesignalsfromtwoarbitraryinputoroutputpositionsgrows\\ninthedistancebetweenpositions,linearlyforConvS2SandlogarithmicallyforByteNet. Thismakes', 'inthedistancebetweenpositions,linearlyforConvS2SandlogarithmicallyforByteNet. Thismakes\\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\\nreducedtoaconstantnumberofoperations, albeitatthecostofreducedeffectiveresolutiondue\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribedinsection3.2.\\nSelf-attention,sometimescalledintra-attentionisanattentionmechanismrelatingdifferentpositions', 'Self-attention,sometimescalledintra-attentionisanattentionmechanismrelatingdifferentpositions\\nofasinglesequenceinordertocomputearepresentationofthesequence. Self-attentionhasbeen\\nusedsuccessfullyinavarietyoftasksincludingreadingcomprehension,abstractivesummarization,\\ntextualentailmentandlearningtask-independentsentencerepresentations[4,27,28,22].\\nEnd-to-endmemorynetworksarebasedonarecurrentattentionmechanisminsteadofsequence-', 'End-to-endmemorynetworksarebasedonarecurrentattentionmechanisminsteadofsequence-\\nalignedrecurrenceandhavebeenshowntoperformwellonsimple-languagequestionansweringand\\nlanguagemodelingtasks[34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirelyonself-attentiontocomputerepresentationsofitsinputandoutputwithoutusingsequence-\\nalignedRNNsorconvolution. Inthefollowingsections,wewilldescribetheTransformer,motivate', 'alignedRNNsorconvolution. Inthefollowingsections,wewilldescribetheTransformer,motivate\\nself-attentionanddiscussitsadvantagesovermodelssuchas[17,18]and[9].\\n3 ModelArchitecture\\nMostcompetitiveneuralsequencetransductionmodelshaveanencoder-decoderstructure[5,2,35].\\nHere, the encoder maps an input sequence of symbol representations (x ,...,x ) to a sequence\\n1 n\\nof continuous representations z = (z ,...,z ). Given z, the decoder then generates an output\\n1 n', 'of continuous representations z = (z ,...,z ). Given z, the decoder then generates an output\\n1 n\\nsequence(y ,...,y )ofsymbolsoneelementatatime. Ateachstepthemodelisauto-regressive\\n1 m\\n[10],consumingthepreviouslygeneratedsymbolsasadditionalinputwhengeneratingthenext.\\n2\\nFigure1: TheTransformer-modelarchitecture.\\nTheTransformerfollowsthisoverallarchitectureusingstackedself-attentionandpoint-wise,fully\\nconnectedlayersforboththeencoderanddecoder,shownintheleftandrighthalvesofFigure1,\\nrespectively.', 'connectedlayersforboththeencoderanddecoder,shownintheleftandrighthalvesofFigure1,\\nrespectively.\\n3.1 EncoderandDecoderStacks\\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. Thefirstisamulti-headself-attentionmechanism,andthesecondisasimple,position-\\nwisefullyconnectedfeed-forwardnetwork. Weemployaresidualconnection[11]aroundeachof\\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is', 'the two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\\nLayerNorm(x+Sublayer(x)),whereSublayer(x)isthefunctionimplementedbythesub-layer\\nitself. Tofacilitatetheseresidualconnections,allsub-layersinthemodel,aswellastheembedding\\nlayers,produceoutputsofdimensiond =512.\\nmodel\\nDecoder: ThedecoderisalsocomposedofastackofN =6identicallayers. Inadditiontothetwo\\nsub-layersineachencoderlayer,thedecoderinsertsathirdsub-layer,whichperformsmulti-head', 'sub-layersineachencoderlayer,thedecoderinsertsathirdsub-layer,whichperformsmulti-head\\nattentionovertheoutputoftheencoderstack. Similartotheencoder,weemployresidualconnections\\naroundeachofthesub-layers,followedbylayernormalization. Wealsomodifytheself-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking,combinedwithfactthattheoutputembeddingsareoffsetbyoneposition,ensuresthatthe', 'masking,combinedwithfactthattheoutputembeddingsareoffsetbyoneposition,ensuresthatthe\\npredictionsforpositionicandependonlyontheknownoutputsatpositionslessthani.\\n3.2 Attention\\nAnattentionfunctioncanbedescribedasmappingaqueryandasetofkey-valuepairstoanoutput,\\nwherethequery,keys,values,andoutputareallvectors. Theoutputiscomputedasaweightedsum\\n3\\nScaledDot-ProductAttention Multi-HeadAttention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several', 'Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattentionlayersrunninginparallel.\\nofthevalues,wheretheweightassignedtoeachvalueiscomputedbyacompatibilityfunctionofthe\\nquerywiththecorrespondingkey.\\n3.2.1 ScaledDot-ProductAttention\\nWecallourparticularattention\"ScaledDot-ProductAttention\"(Figure2). Theinputconsistsof\\nqueriesandkeysofdimensiond\\nk\\n,a√ndvaluesofdimensiond\\nv\\n. Wecomputethedotproductsofthe', 'queriesandkeysofdimensiond\\nk\\n,a√ndvaluesofdimensiond\\nv\\n. Wecomputethedotproductsofthe\\nquerywithallkeys,divideeachby d ,andapplyasoftmaxfunctiontoobtaintheweightsonthe\\nk\\nvalues.\\nInpractice,wecomputetheattentionfunctiononasetofqueriessimultaneously,packedtogether\\nintoamatrixQ. ThekeysandvaluesarealsopackedtogetherintomatricesK andV. Wecompute\\nthematrixofoutputsas:\\nQKT\\nAttention(Q,K,V)=softmax( √ )V (1)\\nd\\nk\\nThetwomostcommonlyusedattentionfunctionsareadditiveattention[2],anddot-product(multi-', 'd\\nk\\nThetwomostcommonlyusedattentionfunctionsareadditiveattention[2],anddot-product(multi-\\nplicative)attention. Dot-productattentionisidenticaltoouralgorithm,exceptforthescalingfactor\\nof √1 . Additiveattentioncomputesthecompatibilityfunctionusingafeed-forwardnetworkwith\\ndk\\nasinglehiddenlayer. Whilethetwoaresimilarintheoreticalcomplexity,dot-productattentionis\\nmuchfasterandmorespace-efficientinpractice,sinceitcanbeimplementedusinghighlyoptimized\\nmatrixmultiplicationcode.', 'matrixmultiplicationcode.\\nWhileforsmallvaluesofd thetwomechanismsperformsimilarly,additiveattentionoutperforms\\nk\\ndotproductattentionwithoutscalingforlargervaluesofd [3]. Wesuspectthatforlargevaluesof\\nk\\nd ,thedotproductsgrowlargeinmagnitude,pushingthesoftmaxfunctionintoregionswhereithas\\nk\\nextremelysmallgradients4. Tocounteractthiseffect,wescalethedotproductsby √1 .\\ndk\\n3.2.2 Multi-HeadAttention\\nInsteadofperformingasingleattentionfunctionwithd -dimensionalkeys,valuesandqueries,\\nmodel', 'Insteadofperformingasingleattentionfunctionwithd -dimensionalkeys,valuesandqueries,\\nmodel\\nwefounditbeneficialtolinearlyprojectthequeries,keysandvalueshtimeswithdifferent,learned\\nlinearprojectionstod ,d andd dimensions,respectively. Oneachoftheseprojectedversionsof\\nk k v\\nqueries,keysandvalueswethenperformtheattentionfunctioninparallel,yieldingd -dimensional\\nv\\n4Toillustratewhythedotproductsgetlarge,assumethatthecomponentsofqandkareindependentrandom', 'v\\n4Toillustratewhythedotproductsgetlarge,assumethatthecomponentsofqandkareindependentrandom\\nvariableswithmean0andvariance1.Thentheirdotproduct,q·k=\\n(cid:80)dk\\nq k ,hasmean0andvarianced .\\ni=1 i i k\\n4\\noutput values. These are concatenated and once again projected, resulting in the final values, as\\ndepictedinFigure2.\\nMulti-headattentionallowsthemodeltojointlyattendtoinformationfromdifferentrepresentation\\nsubspacesatdifferentpositions. Withasingleattentionhead,averaginginhibitsthis.', 'subspacesatdifferentpositions. Withasingleattentionhead,averaginginhibitsthis.\\nMultiHead(Q,K,V)=Concat(head ,...,head )WO\\n1 h\\nwherehead =Attention(QWQ,KWK,VWV)\\ni i i i\\nWheretheprojectionsareparametermatricesWQ ∈Rdmodel×dk,WK ∈Rdmodel×dk,WV ∈Rdmodel×dv\\ni i i\\nandWO ∈Rhdv×dmodel.\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\nd =d =d /h=64. Duetothereduceddimensionofeachhead,thetotalcomputationalcost\\nk v model', 'd =d =d /h=64. Duetothereduceddimensionofeachhead,thetotalcomputationalcost\\nk v model\\nissimilartothatofsingle-headattentionwithfulldimensionality.\\n3.2.3 ApplicationsofAttentioninourModel\\nTheTransformerusesmulti-headattentioninthreedifferentways:\\n• In\"encoder-decoderattention\"layers,thequeriescomefromthepreviousdecoderlayer,\\nandthememorykeysandvaluescomefromtheoutputoftheencoder. Thisallowsevery\\npositioninthedecodertoattendoverallpositionsintheinputsequence. Thismimicsthe', 'positioninthedecodertoattendoverallpositionsintheinputsequence. Thismimicsthe\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38,2,9].\\n• Theencodercontainsself-attentionlayers. Inaself-attentionlayerallofthekeys,values\\nandqueriescomefromthesameplace,inthiscase,theoutputofthepreviouslayerinthe\\nencoder. Eachpositionintheencodercanattendtoallpositionsinthepreviouslayerofthe\\nencoder.', 'encoder. Eachpositionintheencodercanattendtoallpositionsinthepreviouslayerofthe\\nencoder.\\n• Similarly,self-attentionlayersinthedecoderalloweachpositioninthedecodertoattendto\\nallpositionsinthedecoderuptoandincludingthatposition. Weneedtopreventleftward\\ninformationflowinthedecodertopreservetheauto-regressiveproperty. Weimplementthis\\ninsideofscaleddot-productattentionbymaskingout(settingto−∞)allvaluesintheinput\\nofthesoftmaxwhichcorrespondtoillegalconnections. SeeFigure2.', 'ofthesoftmaxwhichcorrespondtoillegalconnections. SeeFigure2.\\n3.3 Position-wiseFeed-ForwardNetworks\\nInadditiontoattentionsub-layers,eachofthelayersinourencoderanddecodercontainsafully\\nconnectedfeed-forwardnetwork,whichisappliedtoeachpositionseparatelyandidentically. This\\nconsistsoftwolineartransformationswithaReLUactivationinbetween.\\nFFN(x)=max(0,xW +b )W +b (2)\\n1 1 2 2\\nWhilethelineartransformationsarethesameacrossdifferentpositions,theyusedifferentparameters', '1 1 2 2\\nWhilethelineartransformationsarethesameacrossdifferentpositions,theyusedifferentparameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is d = 512, and the inner-layer has dimensionality\\nmodel\\nd =2048.\\nff\\n3.4 EmbeddingsandSoftmax\\nSimilarlytoothersequencetransductionmodels,weuselearnedembeddingstoconverttheinput\\ntokensandoutputtokenstovectorsofdimensiond . Wealsousetheusuallearnedlineartransfor-', 'tokensandoutputtokenstovectorsofdimensiond . Wealsousetheusuallearnedlineartransfor-\\nmodel\\nmationandsoftmaxfunctiontoconvertthedecoderoutputtopredictednext-tokenprobabilities. In\\nourmodel,wesharethesameweightmatrixbetweenthetwoembeddinglayersandthepre-√softmax\\nlineartransformation,similarto[30]. Intheembeddinglayers,wemultiplythoseweightsby d .\\nmodel\\n5\\nTable1: Maximumpathlengths,per-layercomplexityandminimumnumberofsequentialoperations', 'model\\n5\\nTable1: Maximumpathlengths,per-layercomplexityandminimumnumberofsequentialoperations\\nfordifferentlayertypes. nisthesequencelength,distherepresentationdimension,kisthekernel\\nsizeofconvolutionsandrthesizeoftheneighborhoodinrestrictedself-attention.\\nLayerType ComplexityperLayer Sequential MaximumPathLength\\nOperations\\nSelf-Attention O(n2·d) O(1) O(1)\\nRecurrent O(n·d2) O(n) O(n)\\nConvolutional O(k·n·d2) O(1) O(log (n))\\nk\\nSelf-Attention(restricted) O(r·n·d) O(1) O(n/r)\\n3.5 PositionalEncoding', 'k\\nSelf-Attention(restricted) O(r·n·d) O(1) O(n/r)\\n3.5 PositionalEncoding\\nSinceourmodelcontainsnorecurrenceandnoconvolution,inorderforthemodeltomakeuseofthe\\norderofthesequence,wemustinjectsomeinformationabouttherelativeorabsolutepositionofthe\\ntokensinthesequence. Tothisend,weadd\"positionalencodings\"totheinputembeddingsatthe\\nbottomsoftheencoderanddecoderstacks. Thepositionalencodingshavethesamedimensiond\\nmodel\\nastheembeddings,sothatthetwocanbesummed. Therearemanychoicesofpositionalencodings,', 'model\\nastheembeddings,sothatthetwocanbesummed. Therearemanychoicesofpositionalencodings,\\nlearnedandfixed[9].\\nInthiswork,weusesineandcosinefunctionsofdifferentfrequencies:\\nPE =sin(pos/100002i/dmodel)\\n(pos,2i)\\nPE =cos(pos/100002i/dmodel)\\n(pos,2i+1)\\nwhereposisthepositionandiisthedimension. Thatis,eachdimensionofthepositionalencoding\\ncorrespondstoasinusoid. Thewavelengthsformageometricprogressionfrom2πto10000·2π. We\\nchosethisfunctionbecausewehypothesizeditwouldallowthemodeltoeasilylearntoattendby', 'chosethisfunctionbecausewehypothesizeditwouldallowthemodeltoeasilylearntoattendby\\nrelativepositions,sinceforanyfixedoffsetk,PE canberepresentedasalinearfunctionof\\npos+k\\nPE .\\npos\\nWealsoexperimentedwithusinglearnedpositionalembeddings[9]instead,andfoundthatthetwo\\nversionsproducednearlyidenticalresults(seeTable3row(E)).Wechosethesinusoidalversion\\nbecauseitmayallowthemodeltoextrapolatetosequencelengthslongerthantheonesencountered\\nduringtraining.\\n4 WhySelf-Attention', 'duringtraining.\\n4 WhySelf-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntionallayerscommonlyusedformappingonevariable-lengthsequenceofsymbolrepresentations\\n(x ,...,x ) to another sequence of equal length (z ,...,z ), with x ,z ∈ Rd, such as a hidden\\n1 n 1 n i i\\nlayerinatypicalsequencetransductionencoderordecoder. Motivatingouruseofself-attentionwe\\nconsiderthreedesiderata.', 'considerthreedesiderata.\\nOneisthetotalcomputationalcomplexityperlayer. Anotheristheamountofcomputationthatcan\\nbeparallelized,asmeasuredbytheminimumnumberofsequentialoperationsrequired.\\nThethirdisthepathlengthbetweenlong-rangedependenciesinthenetwork. Learninglong-range\\ndependenciesisakeychallengeinmanysequencetransductiontasks. Onekeyfactoraffectingthe\\nabilitytolearnsuchdependenciesisthelengthofthepathsforwardandbackwardsignalshaveto', 'abilitytolearnsuchdependenciesisthelengthofthepathsforwardandbackwardsignalshaveto\\ntraverseinthenetwork. Theshorterthesepathsbetweenanycombinationofpositionsintheinput\\nandoutputsequences,theeasieritistolearnlong-rangedependencies[12]. Hencewealsocompare\\nthemaximumpathlengthbetweenanytwoinputandoutputpositionsinnetworkscomposedofthe\\ndifferentlayertypes.\\nAsnotedinTable1,aself-attentionlayerconnectsallpositionswithaconstantnumberofsequentially', 'AsnotedinTable1,aself-attentionlayerconnectsallpositionswithaconstantnumberofsequentially\\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\ncomputationalcomplexity,self-attentionlayersarefasterthanrecurrentlayerswhenthesequence\\n6\\nlength n is smaller than the representation dimensionality d, which is most often the case with\\nsentencerepresentationsusedbystate-of-the-artmodelsinmachinetranslations,suchasword-piece', 'sentencerepresentationsusedbystate-of-the-artmodelsinmachinetranslations,suchasword-piece\\n[38]andbyte-pair[31]representations. Toimprovecomputationalperformancefortasksinvolving\\nverylongsequences,self-attentioncouldberestrictedtoconsideringonlyaneighborhoodofsizerin\\ntheinputsequencecenteredaroundtherespectiveoutputposition. Thiswouldincreasethemaximum\\npathlengthtoO(n/r). Weplantoinvestigatethisapproachfurtherinfuturework.', 'pathlengthtoO(n/r). Weplantoinvestigatethisapproachfurtherinfuturework.\\nAsingleconvolutionallayerwithkernelwidthk <ndoesnotconnectallpairsofinputandoutput\\npositions. DoingsorequiresastackofO(n/k)convolutionallayersinthecaseofcontiguouskernels,\\norO(log (n))inthecaseofdilatedconvolutions[18], increasingthelengthofthelongestpaths\\nk\\nbetweenanytwopositionsinthenetwork. Convolutionallayersaregenerallymoreexpensivethan', 'k\\nbetweenanytwopositionsinthenetwork. Convolutionallayersaregenerallymoreexpensivethan\\nrecurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity\\nconsiderably, toO(k·n·d+n·d2). Evenwithk = n, however, thecomplexityofaseparable\\nconvolutionisequaltothecombinationofaself-attentionlayerandapoint-wisefeed-forwardlayer,\\ntheapproachwetakeinourmodel.\\nAssidebenefit,self-attentioncouldyieldmoreinterpretablemodels.Weinspectattentiondistributions', 'Assidebenefit,self-attentioncouldyieldmoreinterpretablemodels.Weinspectattentiondistributions\\nfromourmodelsandpresentanddiscussexamplesintheappendix. Notonlydoindividualattention\\nheadsclearlylearntoperformdifferenttasks,manyappeartoexhibitbehaviorrelatedtothesyntactic\\nandsemanticstructureofthesentences.\\n5 Training\\nThissectiondescribesthetrainingregimeforourmodels.\\n5.1 TrainingDataandBatching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million', 'We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentencepairs. Sentenceswereencodedusingbyte-pairencoding[3],whichhasasharedsource-\\ntargetvocabularyofabout37000tokens. ForEnglish-French,weusedthesignificantlylargerWMT\\n2014English-Frenchdatasetconsistingof36Msentencesandsplittokensintoa32000word-piece\\nvocabulary[38].Sentencepairswerebatchedtogetherbyapproximatesequencelength.Eachtraining', 'vocabulary[38].Sentencepairswerebatchedtogetherbyapproximatesequencelength.Eachtraining\\nbatchcontainedasetofsentencepairscontainingapproximately25000sourcetokensand25000\\ntargettokens.\\n5.2 HardwareandSchedule\\nWetrainedourmodelsononemachinewith8NVIDIAP100GPUs. Forourbasemodelsusing\\nthehyperparametersdescribedthroughoutthepaper,eachtrainingsteptookabout0.4seconds. We\\ntrainedthebasemodelsforatotalof100,000stepsor12hours. Forourbigmodels,(describedonthe', 'trainedthebasemodelsforatotalof100,000stepsor12hours. Forourbigmodels,(describedonthe\\nbottomlineoftable3),steptimewas1.0seconds. Thebigmodelsweretrainedfor300,000steps\\n(3.5days).\\n5.3 Optimizer\\nWeusedtheAdamoptimizer[20]withβ =0.9,β =0.98andϵ=10−9. Wevariedthelearning\\n1 2\\nrateoverthecourseoftraining,accordingtotheformula:\\nlrate=d−0.5 ·min(step_num−0.5,step_num·warmup_steps−1.5) (3)\\nmodel\\nThiscorrespondstoincreasingthelearningratelinearlyforthefirstwarmup_stepstrainingsteps,', 'model\\nThiscorrespondstoincreasingthelearningratelinearlyforthefirstwarmup_stepstrainingsteps,\\nanddecreasingitthereafterproportionallytotheinversesquarerootofthestepnumber. Weused\\nwarmup_steps=4000.\\n5.4 Regularization\\nWeemploythreetypesofregularizationduringtraining:\\n7\\nTable2: TheTransformerachievesbetterBLEUscoresthanpreviousstate-of-the-artmodelsonthe\\nEnglish-to-GermanandEnglish-to-Frenchnewstest2014testsatafractionofthetrainingcost.\\nBLEU TrainingCost(FLOPs)\\nModel\\nEN-DE EN-FR EN-DE EN-FR', 'BLEU TrainingCost(FLOPs)\\nModel\\nEN-DE EN-FR EN-DE EN-FR\\nByteNet[18] 23.75\\nDeep-Att+PosUnk[39] 39.2 1.0·1020\\nGNMT+RL[38] 24.6 39.92 2.3·1019 1.4·1020\\nConvS2S[9] 25.16 40.46 9.6·1018 1.5·1020\\nMoE[32] 26.03 40.56 2.0·1019 1.2·1020\\nDeep-Att+PosUnkEnsemble[39] 40.4 8.0·1020\\nGNMT+RLEnsemble[38] 26.30 41.16 1.8·1020 1.1·1021\\nConvS2SEnsemble[9] 26.36 41.29 7.7·1019 1.2·1021\\nTransformer(basemodel) 27.3 38.1 3.3·1018\\nTransformer(big) 28.4 41.8 2.3·1019', 'Transformer(basemodel) 27.3 38.1 3.3·1018\\nTransformer(big) 28.4 41.8 2.3·1019\\nResidualDropout Weapplydropout[33]totheoutputofeachsub-layer,beforeitisaddedtothe\\nsub-layerinputandnormalized. Inaddition,weapplydropouttothesumsoftheembeddingsandthe\\npositionalencodingsinboththeencoderanddecoderstacks. Forthebasemodel,weusearateof\\nP =0.1.\\ndrop\\nLabelSmoothing Duringtraining,weemployedlabelsmoothingofvalueϵ = 0.1[36]. This\\nls', 'P =0.1.\\ndrop\\nLabelSmoothing Duringtraining,weemployedlabelsmoothingofvalueϵ = 0.1[36]. This\\nls\\nhurtsperplexity,asthemodellearnstobemoreunsure,butimprovesaccuracyandBLEUscore.\\n6 Results\\n6.1 MachineTranslation\\nOntheWMT2014English-to-Germantranslationtask,thebigtransformermodel(Transformer(big)\\ninTable2)outperformsthebestpreviouslyreportedmodels(includingensembles)bymorethan2.0\\nBLEU,establishinganewstate-of-the-artBLEUscoreof28.4. Theconfigurationofthismodelis', 'BLEU,establishinganewstate-of-the-artBLEUscoreof28.4. Theconfigurationofthismodelis\\nlistedinthebottomlineofTable3. Trainingtook3.5dayson8P100GPUs. Evenourbasemodel\\nsurpassesallpreviouslypublishedmodelsandensembles,atafractionofthetrainingcostofanyof\\nthecompetitivemodels.\\nOntheWMT2014English-to-Frenchtranslationtask,ourbigmodelachievesaBLEUscoreof41.0,\\noutperformingallofthepreviouslypublishedsinglemodels,atlessthan1/4thetrainingcostofthe', 'outperformingallofthepreviouslypublishedsinglemodels,atlessthan1/4thetrainingcostofthe\\npreviousstate-of-the-artmodel. TheTransformer(big)modeltrainedforEnglish-to-Frenchused\\ndropoutrateP =0.1,insteadof0.3.\\ndrop\\nForthebasemodels,weusedasinglemodelobtainedbyaveragingthelast5checkpoints,which\\nwerewrittenat10-minuteintervals. Forthebigmodels,weaveragedthelast20checkpoints. We\\nusedbeamsearchwithabeamsizeof4andlengthpenaltyα = 0.6[38]. Thesehyperparameters', 'usedbeamsearchwithabeamsizeof4andlengthpenaltyα = 0.6[38]. Thesehyperparameters\\nwerechosenafterexperimentationonthedevelopmentset. Wesetthemaximumoutputlengthduring\\ninferencetoinputlength+50,butterminateearlywhenpossible[38].\\nTable2summarizesourresultsandcomparesourtranslationqualityandtrainingcoststoothermodel\\narchitecturesfromtheliterature. Weestimatethenumberoffloatingpointoperationsusedtotraina\\nmodelbymultiplyingthetrainingtime,thenumberofGPUsused,andanestimateofthesustained', 'modelbymultiplyingthetrainingtime,thenumberofGPUsused,andanestimateofthesustained\\nsingle-precisionfloating-pointcapacityofeachGPU5.\\n6.2 ModelVariations\\nToevaluatetheimportanceofdifferentcomponentsoftheTransformer,wevariedourbasemodel\\nindifferentways,measuringthechangeinperformanceonEnglish-to-Germantranslationonthe\\n5Weusedvaluesof2.8,3.7,6.0and9.5TFLOPSforK80,K40,M40andP100,respectively.\\n8\\nTable3: VariationsontheTransformerarchitecture. Unlistedvaluesareidenticaltothoseofthebase', '8\\nTable3: VariationsontheTransformerarchitecture. Unlistedvaluesareidenticaltothoseofthebase\\nmodel. AllmetricsareontheEnglish-to-Germantranslationdevelopmentset,newstest2013. Listed\\nperplexitiesareper-wordpiece,accordingtoourbyte-pairencoding,andshouldnotbecomparedto\\nper-wordperplexities.\\ntrain PPL BLEU params\\nN d d h d d P ϵ\\nmodel ff k v drop ls steps (dev) (dev) ×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n(A)\\n16 32 32 4.91 25.8', '1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n(A)\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n16 5.16 25.1 58\\n(B)\\n32 5.01 25.4 60\\n2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n(C) 256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n0.0 5.77 24.6\\n0.2 4.95 25.5\\n(D)\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positionalembeddinginsteadofsinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\ndevelopmentset,newstest2013. Weusedbeamsearchasdescribedintheprevioussection,butno', 'developmentset,newstest2013. Weusedbeamsearchasdescribedintheprevioussection,butno\\ncheckpointaveraging. WepresenttheseresultsinTable3.\\nInTable3rows(A),wevarythenumberofattentionheadsandtheattentionkeyandvaluedimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattentionis0.9BLEUworsethanthebestsetting,qualityalsodropsoffwithtoomanyheads.\\nInTable3rows(B),weobservethatreducingtheattentionkeysized hurtsmodelquality. This\\nk', 'InTable3rows(B),weobservethatreducingtheattentionkeysized hurtsmodelquality. This\\nk\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunctionthandotproductmaybebeneficial. Wefurtherobserveinrows(C)and(D)that,asexpected,\\nbiggermodelsarebetter,anddropoutisveryhelpfulinavoidingover-fitting.Inrow(E)wereplaceour\\nsinusoidalpositionalencodingwithlearnedpositionalembeddings[9],andobservenearlyidentical\\nresultstothebasemodel.\\n6.3 EnglishConstituencyParsing', 'resultstothebasemodel.\\n6.3 EnglishConstituencyParsing\\nToevaluateiftheTransformercangeneralizetoothertasksweperformedexperimentsonEnglish\\nconstituencyparsing. Thistaskpresentsspecificchallenges: theoutputissubjecttostrongstructural\\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\\nmodelshavenotbeenabletoattainstate-of-the-artresultsinsmall-dataregimes[37].\\nWetraineda4-layertransformerwithd =1024ontheWallStreetJournal(WSJ)portionofthe\\nmodel', 'Wetraineda4-layertransformerwithd =1024ontheWallStreetJournal(WSJ)portionofthe\\nmodel\\nPennTreebank[25],about40Ktrainingsentences. Wealsotraineditinasemi-supervisedsetting,\\nusingthelargerhigh-confidenceandBerkleyParsercorporafromwithapproximately17Msentences\\n[37]. Weusedavocabularyof16KtokensfortheWSJonlysettingandavocabularyof32Ktokens\\nforthesemi-supervisedsetting.\\nWeperformedonlyasmallnumberofexperimentstoselectthedropout,bothattentionandresidual', 'Weperformedonlyasmallnumberofexperimentstoselectthedropout,bothattentionandresidual\\n(section5.4),learningratesandbeamsizeontheSection22developmentset,allotherparameters\\nremained unchanged from the English-to-German base translation model. During inference, we\\n9\\nTable4: TheTransformergeneralizeswelltoEnglishconstituencyparsing(ResultsareonSection23\\nofWSJ)\\nParser Training WSJ23F1\\nVinyals&Kaiserelal. (2014)[37] WSJonly,discriminative 88.3\\nPetrovetal. (2006)[29] WSJonly,discriminative 90.4', 'Petrovetal. (2006)[29] WSJonly,discriminative 90.4\\nZhuetal. (2013)[40] WSJonly,discriminative 90.4\\nDyeretal. (2016)[8] WSJonly,discriminative 91.7\\nTransformer(4layers) WSJonly,discriminative 91.3\\nZhuetal. (2013)[40] semi-supervised 91.3\\nHuang&Harper(2009)[14] semi-supervised 91.3\\nMcCloskyetal. (2006)[26] semi-supervised 92.1\\nVinyals&Kaiserelal. (2014)[37] semi-supervised 92.1\\nTransformer(4layers) semi-supervised 92.7\\nLuongetal. (2015)[23] multi-task 93.0\\nDyeretal. (2016)[8] generative 93.3', 'Luongetal. (2015)[23] multi-task 93.0\\nDyeretal. (2016)[8] generative 93.3\\nincreasedthemaximumoutputlengthtoinputlength+300. Weusedabeamsizeof21andα=0.3\\nforbothWSJonlyandthesemi-supervisedsetting.\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\\nprisinglywell,yieldingbetterresultsthanallpreviouslyreportedmodelswiththeexceptionofthe\\nRecurrentNeuralNetworkGrammar[8].\\nIncontrasttoRNNsequence-to-sequencemodels[37],theTransformeroutperformstheBerkeley-', 'IncontrasttoRNNsequence-to-sequencemodels[37],theTransformeroutperformstheBerkeley-\\nParser[29]evenwhentrainingonlyontheWSJtrainingsetof40Ksentences.\\n7 Conclusion\\nInthiswork,wepresentedtheTransformer,thefirstsequencetransductionmodelbasedentirelyon\\nattention,replacingtherecurrentlayersmostcommonlyusedinencoder-decoderarchitectureswith\\nmulti-headedself-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based', 'For translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-Frenchtranslationtasks,weachieveanewstateoftheart. Intheformertaskourbest\\nmodeloutperformsevenallpreviouslyreportedensembles.\\nWeareexcitedaboutthefutureofattention-basedmodelsandplantoapplythemtoothertasks. We\\nplantoextendtheTransformertoproblemsinvolvinginputandoutputmodalitiesotherthantextand', 'plantoextendtheTransformertoproblemsinvolvinginputandoutputmodalitiesotherthantextand\\ntoinvestigatelocal,restrictedattentionmechanismstoefficientlyhandlelargeinputsandoutputs\\nsuchasimages,audioandvideo. Makinggenerationlesssequentialisanotherresearchgoalsofours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor.\\nAcknowledgements WearegratefultoNalKalchbrennerandStephanGouwsfortheirfruitful\\ncomments,correctionsandinspiration.', 'comments,correctionsandinspiration.\\nReferences\\n[1] JimmyLeiBa,JamieRyanKiros,andGeoffreyEHinton. Layernormalization. arXivpreprint\\narXiv:1607.06450,2016.\\n[2] DzmitryBahdanau,KyunghyunCho,andYoshuaBengio. Neuralmachinetranslationbyjointly\\nlearningtoalignandtranslate. CoRR,abs/1409.0473,2014.\\n[3] DennyBritz,AnnaGoldie,Minh-ThangLuong,andQuocV.Le. Massiveexplorationofneural\\nmachinetranslationarchitectures. CoRR,abs/1703.03906,2017.', 'machinetranslationarchitectures. CoRR,abs/1703.03906,2017.\\n[4] JianpengCheng,LiDong,andMirellaLapata. Longshort-termmemory-networksformachine\\nreading. arXivpreprintarXiv:1601.06733,2016.\\n10\\n[5] KyunghyunCho,BartvanMerrienboer,CaglarGulcehre,FethiBougares,HolgerSchwenk,\\nandYoshuaBengio. Learningphraserepresentationsusingrnnencoder-decoderforstatistical\\nmachinetranslation. CoRR,abs/1406.1078,2014.\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv', '[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprintarXiv:1610.02357,2016.\\n[7] JunyoungChung,ÇaglarGülçehre,KyunghyunCho,andYoshuaBengio. Empiricalevaluation\\nofgatedrecurrentneuralnetworksonsequencemodeling. CoRR,abs/1412.3555,2014.\\n[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\\nnetworkgrammars. InProc.ofNAACL,2016.\\n[9] JonasGehring,MichaelAuli,DavidGrangier,DenisYarats,andYannN.Dauphin. Convolu-', '[9] JonasGehring,MichaelAuli,DavidGrangier,DenisYarats,andYannN.Dauphin. Convolu-\\ntionalsequencetosequencelearning. arXivpreprintarXiv:1705.03122v2,2017.\\n[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850,2013.\\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition,pages770–778,2016.', 'Recognition,pages770–778,2016.\\n[12] SeppHochreiter,YoshuaBengio,PaoloFrasconi,andJürgenSchmidhuber. Gradientflowin\\nrecurrentnets: thedifficultyoflearninglong-termdependencies,2001.\\n[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\\n9(8):1735–1780,1997.\\n[14] ZhongqiangHuangandMaryHarper. Self-trainingPCFGgrammarswithlatentannotations\\nacrosslanguages. InProceedingsofthe2009ConferenceonEmpiricalMethodsinNatural\\nLanguageProcessing,pages832–841.ACL,August2009.', 'LanguageProcessing,pages832–841.ACL,August2009.\\n[15] RafalJozefowicz,OriolVinyals,MikeSchuster,NoamShazeer,andYonghuiWu. Exploring\\nthelimitsoflanguagemodeling. arXivpreprintarXiv:1602.02410,2016.\\n[16] ŁukaszKaiserandSamyBengio. Canactivememoryreplaceattention? InAdvancesinNeural\\nInformationProcessingSystems,(NIPS),2016.\\n[17] ŁukaszKaiserandIlyaSutskever. NeuralGPUslearnalgorithms. InInternationalConference\\nonLearningRepresentations(ICLR),2016.', 'onLearningRepresentations(ICLR),2016.\\n[18] NalKalchbrenner,LasseEspeholt,KarenSimonyan,AaronvandenOord,AlexGraves,andKo-\\nrayKavukcuoglu.Neuralmachinetranslationinlineartime.arXivpreprintarXiv:1610.10099v2,\\n2017.\\n[19] YoonKim,CarlDenton,LuongHoang,andAlexanderM.Rush. Structuredattentionnetworks.\\nInInternationalConferenceonLearningRepresentations,2017.\\n[20] DiederikKingmaandJimmyBa. Adam: Amethodforstochasticoptimization. InICLR,2015.', '[20] DiederikKingmaandJimmyBa. Adam: Amethodforstochasticoptimization. InICLR,2015.\\n[21] OleksiiKuchaievandBorisGinsburg. FactorizationtricksforLSTMnetworks. arXivpreprint\\narXiv:1703.10722,2017.\\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130,2017.\\n[23] Minh-ThangLuong,QuocV.Le,IlyaSutskever,OriolVinyals,andLukaszKaiser. Multi-task', '[23] Minh-ThangLuong,QuocV.Le,IlyaSutskever,OriolVinyals,andLukaszKaiser. Multi-task\\nsequencetosequencelearning. arXivpreprintarXiv:1511.06114,2015.\\n[24] Minh-ThangLuong,HieuPham,andChristopherDManning. Effectiveapproachestoattention-\\nbasedneuralmachinetranslation. arXivpreprintarXiv:1508.04025,2015.\\n11\\n[25] MitchellPMarcus,MaryAnnMarcinkiewicz,andBeatriceSantorini.Buildingalargeannotated\\ncorpusofenglish: Thepenntreebank. Computationallinguistics,19(2):313–330,1993.', 'corpusofenglish: Thepenntreebank. Computationallinguistics,19(2):313–330,1993.\\n[26] DavidMcClosky,EugeneCharniak,andMarkJohnson. Effectiveself-trainingforparsing. In\\nProceedingsoftheHumanLanguageTechnologyConferenceoftheNAACL,MainConference,\\npages152–159.ACL,June2006.\\n[27] AnkurParikh,OscarTäckström,DipanjanDas,andJakobUszkoreit. Adecomposableattention\\nmodel. InEmpiricalMethodsinNaturalLanguageProcessing,2016.\\n[28] RomainPaulus,CaimingXiong,andRichardSocher. Adeepreinforcedmodelforabstractive', '[28] RomainPaulus,CaimingXiong,andRichardSocher. Adeepreinforcedmodelforabstractive\\nsummarization. arXivpreprintarXiv:1705.04304,2017.\\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputationalLinguisticsand44thAnnualMeetingoftheACL,pages433–440.ACL,July\\n2006.\\n[30] OfirPressandLiorWolf. Usingtheoutputembeddingtoimprovelanguagemodels. arXiv', '2006.\\n[30] OfirPressandLiorWolf. Usingtheoutputembeddingtoimprovelanguagemodels. arXiv\\npreprintarXiv:1608.05859,2016.\\n[31] RicoSennrich,BarryHaddow,andAlexandraBirch. Neuralmachinetranslationofrarewords\\nwithsubwordunits. arXivpreprintarXiv:1508.07909,2015.\\n[32] NoamShazeer,AzaliaMirhoseini,KrzysztofMaziarz,AndyDavis,QuocLe,GeoffreyHinton,\\nandJeffDean. Outrageouslylargeneuralnetworks: Thesparsely-gatedmixture-of-experts\\nlayer. arXivpreprintarXiv:1701.06538,2017.', 'layer. arXivpreprintarXiv:1701.06538,2017.\\n[33] NitishSrivastava,GeoffreyEHinton,AlexKrizhevsky,IlyaSutskever,andRuslanSalakhutdi-\\nnov. Dropout: asimplewaytopreventneuralnetworksfromoverfitting. JournalofMachine\\nLearningResearch,15(1):1929–1958,2014.\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. InC.Cortes, N.D.Lawrence, D.D.Lee, M.Sugiyama, andR.Garnett, editors,\\nAdvancesinNeuralInformationProcessingSystems28,pages2440–2448.CurranAssociates,', 'AdvancesinNeuralInformationProcessingSystems28,pages2440–2448.CurranAssociates,\\nInc.,2015.\\n[35] IlyaSutskever,OriolVinyals,andQuocVVLe. Sequencetosequencelearningwithneural\\nnetworks. InAdvancesinNeuralInformationProcessingSystems,pages3104–3112,2014.\\n[36] ChristianSzegedy,VincentVanhoucke,SergeyIoffe,JonathonShlens,andZbigniewWojna.\\nRethinkingtheinceptionarchitectureforcomputervision. CoRR,abs/1512.00567,2015.\\n[37] Vinyals&Kaiser, Koo, Petrov, Sutskever, andHinton. Grammarasaforeignlanguage. In', '[37] Vinyals&Kaiser, Koo, Petrov, Sutskever, andHinton. Grammarasaforeignlanguage. In\\nAdvancesinNeuralInformationProcessingSystems,2015.\\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey,MaximKrikun,YuanCao,QinGao,KlausMacherey,etal. Google’sneuralmachine\\ntranslationsystem: Bridgingthegapbetweenhumanandmachinetranslation. arXivpreprint\\narXiv:1609.08144,2016.\\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with', '[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forwardconnectionsforneuralmachinetranslation. CoRR,abs/1606.04199,2016.\\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\\nshift-reduceconstituentparsing. InProceedingsofthe51stAnnualMeetingoftheACL(Volume\\n1: LongPapers),pages434–443.ACL,August2013.\\n12\\nInput-Input Layer5\\nAttentionVisualizations\\ntI\\ntI\\nsi\\nsi\\nni\\nni\\nsiht\\nsiht\\ntirips\\ntirips\\ntaht\\ntaht\\na\\na\\nytirojam\\nytirojam', 'AttentionVisualizations\\ntI\\ntI\\nsi\\nsi\\nni\\nni\\nsiht\\nsiht\\ntirips\\ntirips\\ntaht\\ntaht\\na\\na\\nytirojam\\nytirojam\\nfo\\nfo\\nnaciremA\\nnaciremA\\nstnemnrevog\\nstnemnrevog\\nevah\\nevah\\ndessap\\ndessap\\nwen\\nwen\\nswal\\nswal\\necnis\\necnis\\n9002\\n9002\\ngnikam\\ngnikam\\neht\\neht\\nnoitartsiger\\nnoitartsiger\\nro\\nro\\ngnitov\\ngnitov\\nssecorp\\nssecorp\\nerom\\nerom\\ntluciffid\\ntluciffid\\n.\\n.\\n>SOE<\\n>SOE<\\n>dap<\\n>dap<\\n>dap<\\n>dap<\\n>dap<\\n>dap<\\n>dap<\\n>dap<\\n>dap<\\n>dap<\\n>dap<\\n>dap<', 'tluciffid\\n.\\n.\\n>SOE<\\n>SOE<\\n>dap<\\n>dap<\\n>dap<\\n>dap<\\n>dap<\\n>dap<\\n>dap<\\n>dap<\\n>dap<\\n>dap<\\n>dap<\\n>dap<\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoderself-attentioninlayer5of6. Manyoftheattentionheadsattendtoadistantdependencyof\\ntheverb‘making’,completingthephrase‘making...moredifficult’. Attentionshereshownonlyfor\\ntheword‘making’. Differentcolorsrepresentdifferentheads. Bestviewedincolor.\\n13\\nInput-Input Layer5\\nehT\\nehT\\nwaL\\nwaL\\nlliw\\nlliw\\nreven\\nreven', '13\\nInput-Input Layer5\\nehT\\nehT\\nwaL\\nwaL\\nlliw\\nlliw\\nreven\\nreven\\neb\\neb\\ntcefrep\\ntcefrep\\n,\\n,\\ntub\\ntub\\nsti\\nsti\\nnoitacilppa\\nnoitacilppa\\ndluohs\\ndluohs\\neb\\neb\\ntsuj\\ntsuj\\n-\\n-\\nsiht\\nsiht\\nsi\\nsi\\ntahw\\ntahw\\new\\new\\nera\\nera\\ngnissim\\ngnissim\\n,\\n,\\nni\\nni\\nym\\nym\\nnoinipo\\nnoinipo\\n.\\n.\\n>SOE<\\n>SOE<\\n>dap<\\n>dap<\\nInput-Input Layer5\\nehT\\nehT\\nwaL\\nwaL\\nlliw\\nlliw\\nreven\\nreven\\neb\\neb\\ntcefrep\\ntcefrep\\n,\\n,\\ntub\\ntub\\nsti\\nsti\\nnoitacilppa\\nnoitacilppa\\ndluohs\\ndluohs\\neb\\neb\\ntsuj\\ntsuj\\n-\\n-\\nsiht\\nsiht\\nsi\\nsi\\ntahw\\ntahw\\new\\new\\nera\\nera\\ngnissim\\ngnissim\\n,\\n,\\nni\\nni', 'dluohs\\ndluohs\\neb\\neb\\ntsuj\\ntsuj\\n-\\n-\\nsiht\\nsiht\\nsi\\nsi\\ntahw\\ntahw\\new\\new\\nera\\nera\\ngnissim\\ngnissim\\n,\\n,\\nni\\nni\\nym\\nym\\nnoinipo\\nnoinipo\\n.\\n.\\n>SOE<\\n>SOE<\\n>dap<\\n>dap<\\nFigure4: Twoattentionheads,alsoinlayer5of6,apparentlyinvolvedinanaphoraresolution. Top:\\nFullattentionsforhead5. Bottom: Isolatedattentionsfromjusttheword‘its’forattentionheads5\\nand6. Notethattheattentionsareverysharpforthisword.\\n14\\nInput-Input Layer5\\nehT\\nehT\\nwaL\\nwaL\\nlliw\\nlliw\\nreven\\nreven\\neb\\neb\\ntcefrep\\ntcefrep\\n,\\n,\\ntub\\ntub\\nsti\\nsti\\nnoitacilppa', 'ehT\\nehT\\nwaL\\nwaL\\nlliw\\nlliw\\nreven\\nreven\\neb\\neb\\ntcefrep\\ntcefrep\\n,\\n,\\ntub\\ntub\\nsti\\nsti\\nnoitacilppa\\nnoitacilppa\\ndluohs\\ndluohs\\neb\\neb\\ntsuj\\ntsuj\\n-\\n-\\nsiht\\nsiht\\nsi\\nsi\\ntahw\\ntahw\\new\\new\\nera\\nera\\ngnissim\\ngnissim\\n,\\n,\\nni\\nni\\nym\\nym\\nnoinipo\\nnoinipo\\n.\\n.\\n>SOE<\\n>SOE<\\n>dap<\\n>dap<\\nInput-Input Layer5\\nehT\\nehT\\nwaL\\nwaL\\nlliw\\nlliw\\nreven\\nreven\\neb\\neb\\ntcefrep\\ntcefrep\\n,\\n,\\ntub\\ntub\\nsti\\nsti\\nnoitacilppa\\nnoitacilppa\\ndluohs\\ndluohs\\neb\\neb\\ntsuj\\ntsuj\\n-\\n-\\nsiht\\nsiht\\nsi\\nsi\\ntahw\\ntahw\\new\\new\\nera\\nera\\ngnissim\\ngnissim\\n,\\n,\\nni\\nni\\nym\\nym\\nnoinipo\\nnoinipo', 'tsuj\\n-\\n-\\nsiht\\nsiht\\nsi\\nsi\\ntahw\\ntahw\\new\\new\\nera\\nera\\ngnissim\\ngnissim\\n,\\n,\\nni\\nni\\nym\\nym\\nnoinipo\\nnoinipo\\n.\\n.\\n>SOE<\\n>SOE<\\n>dap<\\n>dap<\\nFigure5: Manyoftheattentionheadsexhibitbehaviourthatseemsrelatedtothestructureofthe\\nsentence. Wegivetwosuchexamplesabove,fromtwodifferentheadsfromtheencoderself-attention\\natlayer5of6. Theheadsclearlylearnedtoperformdifferenttasks.\\n15']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_chunks"
      ],
      "metadata": {
        "id": "i3rDMdGAF9Tn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb7e48d5-527e-4f3e-d6e9-354f1b051175"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Providedproperattributionisprovided,Googleherebygrantspermissionto\\nreproducethetablesandfiguresinthispapersolelyforuseinjournalisticor\\nscholarlyworks.\\nAttention Is All You Need\\nAshishVaswani∗ NoamShazeer∗ NikiParmar∗ JakobUszkoreit∗\\nGoogleBrain GoogleBrain GoogleResearch GoogleResearch\\navaswani@google.com noam@google.com nikip@google.com usz@google.com\\nLlionJones∗ AidanN.Gomez∗ † ŁukaszKaiser∗\\nGoogleResearch UniversityofToronto GoogleBrain',\n",
              " 'LlionJones∗ AidanN.Gomez∗ † ŁukaszKaiser∗\\nGoogleResearch UniversityofToronto GoogleBrain\\nllion@google.com aidan@cs.toronto.edu lukaszkaiser@google.com\\nIlliaPolosukhin∗ ‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThedominantsequencetransductionmodelsarebasedoncomplexrecurrentor\\nconvolutionalneuralnetworksthatincludeanencoderandadecoder. Thebest\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,',\n",
              " 'mechanism. We propose a new simple network architecture, the Transformer,\\nbasedsolelyonattentionmechanisms,dispensingwithrecurrenceandconvolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbesuperiorinqualitywhilebeingmoreparallelizableandrequiringsignificantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including',\n",
              " 'to-German translation task, improving over the existing best results, including\\nensembles,byover2BLEU.OntheWMT2014English-to-Frenchtranslationtask,\\nourmodelestablishesanewsingle-modelstate-of-the-artBLEUscoreof41.8after\\ntrainingfor3.5daysoneightGPUs,asmallfractionofthetrainingcostsofthe\\nbestmodelsfromtheliterature. WeshowthattheTransformergeneralizeswellto\\nothertasksbyapplyingitsuccessfullytoEnglishconstituencyparsingbothwith\\nlargeandlimitedtrainingdata.',\n",
              " 'othertasksbyapplyingitsuccessfullytoEnglishconstituencyparsingbothwith\\nlargeandlimitedtrainingdata.\\n∗Equalcontribution.Listingorderisrandom.JakobproposedreplacingRNNswithself-attentionandstarted\\ntheefforttoevaluatethisidea.Ashish,withIllia,designedandimplementedthefirstTransformermodelsand\\nhasbeencruciallyinvolvedineveryaspectofthiswork.Noamproposedscaleddot-productattention,multi-head\\nattentionandtheparameter-freepositionrepresentationandbecametheotherpersoninvolvedinnearlyevery',\n",
              " 'attentionandtheparameter-freepositionrepresentationandbecametheotherpersoninvolvedinnearlyevery\\ndetail.Nikidesigned,implemented,tunedandevaluatedcountlessmodelvariantsinouroriginalcodebaseand\\ntensor2tensor.Llionalsoexperimentedwithnovelmodelvariants,wasresponsibleforourinitialcodebase,and\\nefficientinferenceandvisualizations.LukaszandAidanspentcountlesslongdaysdesigningvariouspartsofand\\nimplementingtensor2tensor,replacingourearliercodebase,greatlyimprovingresultsandmassivelyaccelerating',\n",
              " 'ourresearch.\\n†WorkperformedwhileatGoogleBrain.\\n‡WorkperformedwhileatGoogleResearch.\\n31stConferenceonNeuralInformationProcessingSystems(NIPS2017),LongBeach,CA,USA.\\n3202\\nguA\\n2\\n]LC.sc[\\n7v26730.6071:viXra\\n1 Introduction\\nRecurrentneuralnetworks,longshort-termmemory[13]andgatedrecurrent[7]neuralnetworks\\ninparticular,havebeenfirmlyestablishedasstateoftheartapproachesinsequencemodelingand\\ntransductionproblemssuchaslanguagemodelingandmachinetranslation[35,2,5]. Numerous',\n",
              " 'transductionproblemssuchaslanguagemodelingandmachinetranslation[35,2,5]. Numerous\\neffortshavesincecontinuedtopushtheboundariesofrecurrentlanguagemodelsandencoder-decoder\\narchitectures[38,24,15].\\nRecurrentmodelstypicallyfactorcomputationalongthesymbolpositionsoftheinputandoutput\\nsequences. Aligningthepositionstostepsincomputationtime,theygenerateasequenceofhidden\\nstatesh ,asafunctionoftheprevioushiddenstateh andtheinputforpositiont. Thisinherently\\nt t−1',\n",
              " 'statesh ,asafunctionoftheprevioushiddenstateh andtheinputforpositiont. Thisinherently\\nt t−1\\nsequentialnatureprecludesparallelizationwithintrainingexamples,whichbecomescriticalatlonger\\nsequencelengths,asmemoryconstraintslimitbatchingacrossexamples. Recentworkhasachieved\\nsignificantimprovementsincomputationalefficiencythroughfactorizationtricks[21]andconditional\\ncomputation[32],whilealsoimprovingmodelperformanceincaseofthelatter. Thefundamental\\nconstraintofsequentialcomputation,however,remains.',\n",
              " 'constraintofsequentialcomputation,however,remains.\\nAttentionmechanismshavebecomeanintegralpartofcompellingsequencemodelingandtransduc-\\ntionmodelsinvarioustasks,allowingmodelingofdependencieswithoutregardtotheirdistancein\\ntheinputoroutputsequences[2,19]. Inallbutafewcases[27],however,suchattentionmechanisms\\nareusedinconjunctionwitharecurrentnetwork.\\nInthisworkweproposetheTransformer,amodelarchitectureeschewingrecurrenceandinstead',\n",
              " 'InthisworkweproposetheTransformer,amodelarchitectureeschewingrecurrenceandinstead\\nrelyingentirelyonanattentionmechanismtodrawglobaldependenciesbetweeninputandoutput.\\nTheTransformerallowsforsignificantlymoreparallelizationandcanreachanewstateoftheartin\\ntranslationqualityafterbeingtrainedforaslittleastwelvehoursoneightP100GPUs.\\n2 Background\\nThegoalofreducingsequentialcomputationalsoformsthefoundationoftheExtendedNeuralGPU',\n",
              " '2 Background\\nThegoalofreducingsequentialcomputationalsoformsthefoundationoftheExtendedNeuralGPU\\n[16],ByteNet[18]andConvS2S[9],allofwhichuseconvolutionalneuralnetworksasbasicbuilding\\nblock,computinghiddenrepresentationsinparallelforallinputandoutputpositions.Inthesemodels,\\nthenumberofoperationsrequiredtorelatesignalsfromtwoarbitraryinputoroutputpositionsgrows\\ninthedistancebetweenpositions,linearlyforConvS2SandlogarithmicallyforByteNet. Thismakes',\n",
              " 'inthedistancebetweenpositions,linearlyforConvS2SandlogarithmicallyforByteNet. Thismakes\\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\\nreducedtoaconstantnumberofoperations, albeitatthecostofreducedeffectiveresolutiondue\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribedinsection3.2.\\nSelf-attention,sometimescalledintra-attentionisanattentionmechanismrelatingdifferentpositions',\n",
              " 'Self-attention,sometimescalledintra-attentionisanattentionmechanismrelatingdifferentpositions\\nofasinglesequenceinordertocomputearepresentationofthesequence. Self-attentionhasbeen\\nusedsuccessfullyinavarietyoftasksincludingreadingcomprehension,abstractivesummarization,\\ntextualentailmentandlearningtask-independentsentencerepresentations[4,27,28,22].\\nEnd-to-endmemorynetworksarebasedonarecurrentattentionmechanisminsteadofsequence-',\n",
              " 'End-to-endmemorynetworksarebasedonarecurrentattentionmechanisminsteadofsequence-\\nalignedrecurrenceandhavebeenshowntoperformwellonsimple-languagequestionansweringand\\nlanguagemodelingtasks[34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirelyonself-attentiontocomputerepresentationsofitsinputandoutputwithoutusingsequence-\\nalignedRNNsorconvolution. Inthefollowingsections,wewilldescribetheTransformer,motivate',\n",
              " 'alignedRNNsorconvolution. Inthefollowingsections,wewilldescribetheTransformer,motivate\\nself-attentionanddiscussitsadvantagesovermodelssuchas[17,18]and[9].\\n3 ModelArchitecture\\nMostcompetitiveneuralsequencetransductionmodelshaveanencoder-decoderstructure[5,2,35].\\nHere, the encoder maps an input sequence of symbol representations (x ,...,x ) to a sequence\\n1 n\\nof continuous representations z = (z ,...,z ). Given z, the decoder then generates an output\\n1 n',\n",
              " 'of continuous representations z = (z ,...,z ). Given z, the decoder then generates an output\\n1 n\\nsequence(y ,...,y )ofsymbolsoneelementatatime. Ateachstepthemodelisauto-regressive\\n1 m\\n[10],consumingthepreviouslygeneratedsymbolsasadditionalinputwhengeneratingthenext.\\n2\\nFigure1: TheTransformer-modelarchitecture.\\nTheTransformerfollowsthisoverallarchitectureusingstackedself-attentionandpoint-wise,fully\\nconnectedlayersforboththeencoderanddecoder,shownintheleftandrighthalvesofFigure1,\\nrespectively.',\n",
              " 'connectedlayersforboththeencoderanddecoder,shownintheleftandrighthalvesofFigure1,\\nrespectively.\\n3.1 EncoderandDecoderStacks\\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. Thefirstisamulti-headself-attentionmechanism,andthesecondisasimple,position-\\nwisefullyconnectedfeed-forwardnetwork. Weemployaresidualconnection[11]aroundeachof\\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is',\n",
              " 'the two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\\nLayerNorm(x+Sublayer(x)),whereSublayer(x)isthefunctionimplementedbythesub-layer\\nitself. Tofacilitatetheseresidualconnections,allsub-layersinthemodel,aswellastheembedding\\nlayers,produceoutputsofdimensiond =512.\\nmodel\\nDecoder: ThedecoderisalsocomposedofastackofN =6identicallayers. Inadditiontothetwo\\nsub-layersineachencoderlayer,thedecoderinsertsathirdsub-layer,whichperformsmulti-head',\n",
              " 'sub-layersineachencoderlayer,thedecoderinsertsathirdsub-layer,whichperformsmulti-head\\nattentionovertheoutputoftheencoderstack. Similartotheencoder,weemployresidualconnections\\naroundeachofthesub-layers,followedbylayernormalization. Wealsomodifytheself-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking,combinedwithfactthattheoutputembeddingsareoffsetbyoneposition,ensuresthatthe',\n",
              " 'masking,combinedwithfactthattheoutputembeddingsareoffsetbyoneposition,ensuresthatthe\\npredictionsforpositionicandependonlyontheknownoutputsatpositionslessthani.\\n3.2 Attention\\nAnattentionfunctioncanbedescribedasmappingaqueryandasetofkey-valuepairstoanoutput,\\nwherethequery,keys,values,andoutputareallvectors. Theoutputiscomputedasaweightedsum\\n3\\nScaledDot-ProductAttention Multi-HeadAttention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several',\n",
              " 'Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattentionlayersrunninginparallel.\\nofthevalues,wheretheweightassignedtoeachvalueiscomputedbyacompatibilityfunctionofthe\\nquerywiththecorrespondingkey.\\n3.2.1 ScaledDot-ProductAttention\\nWecallourparticularattention\"ScaledDot-ProductAttention\"(Figure2). Theinputconsistsof\\nqueriesandkeysofdimensiond\\nk\\n,a√ndvaluesofdimensiond\\nv\\n. Wecomputethedotproductsofthe',\n",
              " 'queriesandkeysofdimensiond\\nk\\n,a√ndvaluesofdimensiond\\nv\\n. Wecomputethedotproductsofthe\\nquerywithallkeys,divideeachby d ,andapplyasoftmaxfunctiontoobtaintheweightsonthe\\nk\\nvalues.\\nInpractice,wecomputetheattentionfunctiononasetofqueriessimultaneously,packedtogether\\nintoamatrixQ. ThekeysandvaluesarealsopackedtogetherintomatricesK andV. Wecompute\\nthematrixofoutputsas:\\nQKT\\nAttention(Q,K,V)=softmax( √ )V (1)\\nd\\nk\\nThetwomostcommonlyusedattentionfunctionsareadditiveattention[2],anddot-product(multi-',\n",
              " 'd\\nk\\nThetwomostcommonlyusedattentionfunctionsareadditiveattention[2],anddot-product(multi-\\nplicative)attention. Dot-productattentionisidenticaltoouralgorithm,exceptforthescalingfactor\\nof √1 . Additiveattentioncomputesthecompatibilityfunctionusingafeed-forwardnetworkwith\\ndk\\nasinglehiddenlayer. Whilethetwoaresimilarintheoreticalcomplexity,dot-productattentionis\\nmuchfasterandmorespace-efficientinpractice,sinceitcanbeimplementedusinghighlyoptimized\\nmatrixmultiplicationcode.',\n",
              " 'matrixmultiplicationcode.\\nWhileforsmallvaluesofd thetwomechanismsperformsimilarly,additiveattentionoutperforms\\nk\\ndotproductattentionwithoutscalingforlargervaluesofd [3]. Wesuspectthatforlargevaluesof\\nk\\nd ,thedotproductsgrowlargeinmagnitude,pushingthesoftmaxfunctionintoregionswhereithas\\nk\\nextremelysmallgradients4. Tocounteractthiseffect,wescalethedotproductsby √1 .\\ndk\\n3.2.2 Multi-HeadAttention\\nInsteadofperformingasingleattentionfunctionwithd -dimensionalkeys,valuesandqueries,\\nmodel',\n",
              " 'Insteadofperformingasingleattentionfunctionwithd -dimensionalkeys,valuesandqueries,\\nmodel\\nwefounditbeneficialtolinearlyprojectthequeries,keysandvalueshtimeswithdifferent,learned\\nlinearprojectionstod ,d andd dimensions,respectively. Oneachoftheseprojectedversionsof\\nk k v\\nqueries,keysandvalueswethenperformtheattentionfunctioninparallel,yieldingd -dimensional\\nv\\n4Toillustratewhythedotproductsgetlarge,assumethatthecomponentsofqandkareindependentrandom',\n",
              " 'v\\n4Toillustratewhythedotproductsgetlarge,assumethatthecomponentsofqandkareindependentrandom\\nvariableswithmean0andvariance1.Thentheirdotproduct,q·k=\\n(cid:80)dk\\nq k ,hasmean0andvarianced .\\ni=1 i i k\\n4\\noutput values. These are concatenated and once again projected, resulting in the final values, as\\ndepictedinFigure2.\\nMulti-headattentionallowsthemodeltojointlyattendtoinformationfromdifferentrepresentation\\nsubspacesatdifferentpositions. Withasingleattentionhead,averaginginhibitsthis.',\n",
              " 'subspacesatdifferentpositions. Withasingleattentionhead,averaginginhibitsthis.\\nMultiHead(Q,K,V)=Concat(head ,...,head )WO\\n1 h\\nwherehead =Attention(QWQ,KWK,VWV)\\ni i i i\\nWheretheprojectionsareparametermatricesWQ ∈Rdmodel×dk,WK ∈Rdmodel×dk,WV ∈Rdmodel×dv\\ni i i\\nandWO ∈Rhdv×dmodel.\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\nd =d =d /h=64. Duetothereduceddimensionofeachhead,thetotalcomputationalcost\\nk v model',\n",
              " 'd =d =d /h=64. Duetothereduceddimensionofeachhead,thetotalcomputationalcost\\nk v model\\nissimilartothatofsingle-headattentionwithfulldimensionality.\\n3.2.3 ApplicationsofAttentioninourModel\\nTheTransformerusesmulti-headattentioninthreedifferentways:\\n• In\"encoder-decoderattention\"layers,thequeriescomefromthepreviousdecoderlayer,\\nandthememorykeysandvaluescomefromtheoutputoftheencoder. Thisallowsevery\\npositioninthedecodertoattendoverallpositionsintheinputsequence. Thismimicsthe',\n",
              " 'positioninthedecodertoattendoverallpositionsintheinputsequence. Thismimicsthe\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38,2,9].\\n• Theencodercontainsself-attentionlayers. Inaself-attentionlayerallofthekeys,values\\nandqueriescomefromthesameplace,inthiscase,theoutputofthepreviouslayerinthe\\nencoder. Eachpositionintheencodercanattendtoallpositionsinthepreviouslayerofthe\\nencoder.',\n",
              " 'encoder. Eachpositionintheencodercanattendtoallpositionsinthepreviouslayerofthe\\nencoder.\\n• Similarly,self-attentionlayersinthedecoderalloweachpositioninthedecodertoattendto\\nallpositionsinthedecoderuptoandincludingthatposition. Weneedtopreventleftward\\ninformationflowinthedecodertopreservetheauto-regressiveproperty. Weimplementthis\\ninsideofscaleddot-productattentionbymaskingout(settingto−∞)allvaluesintheinput\\nofthesoftmaxwhichcorrespondtoillegalconnections. SeeFigure2.',\n",
              " 'ofthesoftmaxwhichcorrespondtoillegalconnections. SeeFigure2.\\n3.3 Position-wiseFeed-ForwardNetworks\\nInadditiontoattentionsub-layers,eachofthelayersinourencoderanddecodercontainsafully\\nconnectedfeed-forwardnetwork,whichisappliedtoeachpositionseparatelyandidentically. This\\nconsistsoftwolineartransformationswithaReLUactivationinbetween.\\nFFN(x)=max(0,xW +b )W +b (2)\\n1 1 2 2\\nWhilethelineartransformationsarethesameacrossdifferentpositions,theyusedifferentparameters',\n",
              " '1 1 2 2\\nWhilethelineartransformationsarethesameacrossdifferentpositions,theyusedifferentparameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is d = 512, and the inner-layer has dimensionality\\nmodel\\nd =2048.\\nff\\n3.4 EmbeddingsandSoftmax\\nSimilarlytoothersequencetransductionmodels,weuselearnedembeddingstoconverttheinput\\ntokensandoutputtokenstovectorsofdimensiond . Wealsousetheusuallearnedlineartransfor-',\n",
              " 'tokensandoutputtokenstovectorsofdimensiond . Wealsousetheusuallearnedlineartransfor-\\nmodel\\nmationandsoftmaxfunctiontoconvertthedecoderoutputtopredictednext-tokenprobabilities. In\\nourmodel,wesharethesameweightmatrixbetweenthetwoembeddinglayersandthepre-√softmax\\nlineartransformation,similarto[30]. Intheembeddinglayers,wemultiplythoseweightsby d .\\nmodel\\n5\\nTable1: Maximumpathlengths,per-layercomplexityandminimumnumberofsequentialoperations',\n",
              " 'model\\n5\\nTable1: Maximumpathlengths,per-layercomplexityandminimumnumberofsequentialoperations\\nfordifferentlayertypes. nisthesequencelength,distherepresentationdimension,kisthekernel\\nsizeofconvolutionsandrthesizeoftheneighborhoodinrestrictedself-attention.\\nLayerType ComplexityperLayer Sequential MaximumPathLength\\nOperations\\nSelf-Attention O(n2·d) O(1) O(1)\\nRecurrent O(n·d2) O(n) O(n)\\nConvolutional O(k·n·d2) O(1) O(log (n))\\nk\\nSelf-Attention(restricted) O(r·n·d) O(1) O(n/r)\\n3.5 PositionalEncoding',\n",
              " 'k\\nSelf-Attention(restricted) O(r·n·d) O(1) O(n/r)\\n3.5 PositionalEncoding\\nSinceourmodelcontainsnorecurrenceandnoconvolution,inorderforthemodeltomakeuseofthe\\norderofthesequence,wemustinjectsomeinformationabouttherelativeorabsolutepositionofthe\\ntokensinthesequence. Tothisend,weadd\"positionalencodings\"totheinputembeddingsatthe\\nbottomsoftheencoderanddecoderstacks. Thepositionalencodingshavethesamedimensiond\\nmodel\\nastheembeddings,sothatthetwocanbesummed. Therearemanychoicesofpositionalencodings,',\n",
              " 'model\\nastheembeddings,sothatthetwocanbesummed. Therearemanychoicesofpositionalencodings,\\nlearnedandfixed[9].\\nInthiswork,weusesineandcosinefunctionsofdifferentfrequencies:\\nPE =sin(pos/100002i/dmodel)\\n(pos,2i)\\nPE =cos(pos/100002i/dmodel)\\n(pos,2i+1)\\nwhereposisthepositionandiisthedimension. Thatis,eachdimensionofthepositionalencoding\\ncorrespondstoasinusoid. Thewavelengthsformageometricprogressionfrom2πto10000·2π. We\\nchosethisfunctionbecausewehypothesizeditwouldallowthemodeltoeasilylearntoattendby',\n",
              " 'chosethisfunctionbecausewehypothesizeditwouldallowthemodeltoeasilylearntoattendby\\nrelativepositions,sinceforanyfixedoffsetk,PE canberepresentedasalinearfunctionof\\npos+k\\nPE .\\npos\\nWealsoexperimentedwithusinglearnedpositionalembeddings[9]instead,andfoundthatthetwo\\nversionsproducednearlyidenticalresults(seeTable3row(E)).Wechosethesinusoidalversion\\nbecauseitmayallowthemodeltoextrapolatetosequencelengthslongerthantheonesencountered\\nduringtraining.\\n4 WhySelf-Attention',\n",
              " 'duringtraining.\\n4 WhySelf-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntionallayerscommonlyusedformappingonevariable-lengthsequenceofsymbolrepresentations\\n(x ,...,x ) to another sequence of equal length (z ,...,z ), with x ,z ∈ Rd, such as a hidden\\n1 n 1 n i i\\nlayerinatypicalsequencetransductionencoderordecoder. Motivatingouruseofself-attentionwe\\nconsiderthreedesiderata.',\n",
              " 'considerthreedesiderata.\\nOneisthetotalcomputationalcomplexityperlayer. Anotheristheamountofcomputationthatcan\\nbeparallelized,asmeasuredbytheminimumnumberofsequentialoperationsrequired.\\nThethirdisthepathlengthbetweenlong-rangedependenciesinthenetwork. Learninglong-range\\ndependenciesisakeychallengeinmanysequencetransductiontasks. Onekeyfactoraffectingthe\\nabilitytolearnsuchdependenciesisthelengthofthepathsforwardandbackwardsignalshaveto',\n",
              " 'abilitytolearnsuchdependenciesisthelengthofthepathsforwardandbackwardsignalshaveto\\ntraverseinthenetwork. Theshorterthesepathsbetweenanycombinationofpositionsintheinput\\nandoutputsequences,theeasieritistolearnlong-rangedependencies[12]. Hencewealsocompare\\nthemaximumpathlengthbetweenanytwoinputandoutputpositionsinnetworkscomposedofthe\\ndifferentlayertypes.\\nAsnotedinTable1,aself-attentionlayerconnectsallpositionswithaconstantnumberofsequentially',\n",
              " 'AsnotedinTable1,aself-attentionlayerconnectsallpositionswithaconstantnumberofsequentially\\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\ncomputationalcomplexity,self-attentionlayersarefasterthanrecurrentlayerswhenthesequence\\n6\\nlength n is smaller than the representation dimensionality d, which is most often the case with\\nsentencerepresentationsusedbystate-of-the-artmodelsinmachinetranslations,suchasword-piece',\n",
              " 'sentencerepresentationsusedbystate-of-the-artmodelsinmachinetranslations,suchasword-piece\\n[38]andbyte-pair[31]representations. Toimprovecomputationalperformancefortasksinvolving\\nverylongsequences,self-attentioncouldberestrictedtoconsideringonlyaneighborhoodofsizerin\\ntheinputsequencecenteredaroundtherespectiveoutputposition. Thiswouldincreasethemaximum\\npathlengthtoO(n/r). Weplantoinvestigatethisapproachfurtherinfuturework.',\n",
              " 'pathlengthtoO(n/r). Weplantoinvestigatethisapproachfurtherinfuturework.\\nAsingleconvolutionallayerwithkernelwidthk <ndoesnotconnectallpairsofinputandoutput\\npositions. DoingsorequiresastackofO(n/k)convolutionallayersinthecaseofcontiguouskernels,\\norO(log (n))inthecaseofdilatedconvolutions[18], increasingthelengthofthelongestpaths\\nk\\nbetweenanytwopositionsinthenetwork. Convolutionallayersaregenerallymoreexpensivethan',\n",
              " 'k\\nbetweenanytwopositionsinthenetwork. Convolutionallayersaregenerallymoreexpensivethan\\nrecurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity\\nconsiderably, toO(k·n·d+n·d2). Evenwithk = n, however, thecomplexityofaseparable\\nconvolutionisequaltothecombinationofaself-attentionlayerandapoint-wisefeed-forwardlayer,\\ntheapproachwetakeinourmodel.\\nAssidebenefit,self-attentioncouldyieldmoreinterpretablemodels.Weinspectattentiondistributions',\n",
              " 'Assidebenefit,self-attentioncouldyieldmoreinterpretablemodels.Weinspectattentiondistributions\\nfromourmodelsandpresentanddiscussexamplesintheappendix. Notonlydoindividualattention\\nheadsclearlylearntoperformdifferenttasks,manyappeartoexhibitbehaviorrelatedtothesyntactic\\nandsemanticstructureofthesentences.\\n5 Training\\nThissectiondescribesthetrainingregimeforourmodels.\\n5.1 TrainingDataandBatching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million',\n",
              " 'We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentencepairs. Sentenceswereencodedusingbyte-pairencoding[3],whichhasasharedsource-\\ntargetvocabularyofabout37000tokens. ForEnglish-French,weusedthesignificantlylargerWMT\\n2014English-Frenchdatasetconsistingof36Msentencesandsplittokensintoa32000word-piece\\nvocabulary[38].Sentencepairswerebatchedtogetherbyapproximatesequencelength.Eachtraining',\n",
              " 'vocabulary[38].Sentencepairswerebatchedtogetherbyapproximatesequencelength.Eachtraining\\nbatchcontainedasetofsentencepairscontainingapproximately25000sourcetokensand25000\\ntargettokens.\\n5.2 HardwareandSchedule\\nWetrainedourmodelsononemachinewith8NVIDIAP100GPUs. Forourbasemodelsusing\\nthehyperparametersdescribedthroughoutthepaper,eachtrainingsteptookabout0.4seconds. We\\ntrainedthebasemodelsforatotalof100,000stepsor12hours. Forourbigmodels,(describedonthe',\n",
              " 'trainedthebasemodelsforatotalof100,000stepsor12hours. Forourbigmodels,(describedonthe\\nbottomlineoftable3),steptimewas1.0seconds. Thebigmodelsweretrainedfor300,000steps\\n(3.5days).\\n5.3 Optimizer\\nWeusedtheAdamoptimizer[20]withβ =0.9,β =0.98andϵ=10−9. Wevariedthelearning\\n1 2\\nrateoverthecourseoftraining,accordingtotheformula:\\nlrate=d−0.5 ·min(step_num−0.5,step_num·warmup_steps−1.5) (3)\\nmodel\\nThiscorrespondstoincreasingthelearningratelinearlyforthefirstwarmup_stepstrainingsteps,',\n",
              " 'model\\nThiscorrespondstoincreasingthelearningratelinearlyforthefirstwarmup_stepstrainingsteps,\\nanddecreasingitthereafterproportionallytotheinversesquarerootofthestepnumber. Weused\\nwarmup_steps=4000.\\n5.4 Regularization\\nWeemploythreetypesofregularizationduringtraining:\\n7\\nTable2: TheTransformerachievesbetterBLEUscoresthanpreviousstate-of-the-artmodelsonthe\\nEnglish-to-GermanandEnglish-to-Frenchnewstest2014testsatafractionofthetrainingcost.\\nBLEU TrainingCost(FLOPs)\\nModel\\nEN-DE EN-FR EN-DE EN-FR',\n",
              " 'BLEU TrainingCost(FLOPs)\\nModel\\nEN-DE EN-FR EN-DE EN-FR\\nByteNet[18] 23.75\\nDeep-Att+PosUnk[39] 39.2 1.0·1020\\nGNMT+RL[38] 24.6 39.92 2.3·1019 1.4·1020\\nConvS2S[9] 25.16 40.46 9.6·1018 1.5·1020\\nMoE[32] 26.03 40.56 2.0·1019 1.2·1020\\nDeep-Att+PosUnkEnsemble[39] 40.4 8.0·1020\\nGNMT+RLEnsemble[38] 26.30 41.16 1.8·1020 1.1·1021\\nConvS2SEnsemble[9] 26.36 41.29 7.7·1019 1.2·1021\\nTransformer(basemodel) 27.3 38.1 3.3·1018\\nTransformer(big) 28.4 41.8 2.3·1019',\n",
              " 'Transformer(basemodel) 27.3 38.1 3.3·1018\\nTransformer(big) 28.4 41.8 2.3·1019\\nResidualDropout Weapplydropout[33]totheoutputofeachsub-layer,beforeitisaddedtothe\\nsub-layerinputandnormalized. Inaddition,weapplydropouttothesumsoftheembeddingsandthe\\npositionalencodingsinboththeencoderanddecoderstacks. Forthebasemodel,weusearateof\\nP =0.1.\\ndrop\\nLabelSmoothing Duringtraining,weemployedlabelsmoothingofvalueϵ = 0.1[36]. This\\nls',\n",
              " 'P =0.1.\\ndrop\\nLabelSmoothing Duringtraining,weemployedlabelsmoothingofvalueϵ = 0.1[36]. This\\nls\\nhurtsperplexity,asthemodellearnstobemoreunsure,butimprovesaccuracyandBLEUscore.\\n6 Results\\n6.1 MachineTranslation\\nOntheWMT2014English-to-Germantranslationtask,thebigtransformermodel(Transformer(big)\\ninTable2)outperformsthebestpreviouslyreportedmodels(includingensembles)bymorethan2.0\\nBLEU,establishinganewstate-of-the-artBLEUscoreof28.4. Theconfigurationofthismodelis',\n",
              " 'BLEU,establishinganewstate-of-the-artBLEUscoreof28.4. Theconfigurationofthismodelis\\nlistedinthebottomlineofTable3. Trainingtook3.5dayson8P100GPUs. Evenourbasemodel\\nsurpassesallpreviouslypublishedmodelsandensembles,atafractionofthetrainingcostofanyof\\nthecompetitivemodels.\\nOntheWMT2014English-to-Frenchtranslationtask,ourbigmodelachievesaBLEUscoreof41.0,\\noutperformingallofthepreviouslypublishedsinglemodels,atlessthan1/4thetrainingcostofthe',\n",
              " 'outperformingallofthepreviouslypublishedsinglemodels,atlessthan1/4thetrainingcostofthe\\npreviousstate-of-the-artmodel. TheTransformer(big)modeltrainedforEnglish-to-Frenchused\\ndropoutrateP =0.1,insteadof0.3.\\ndrop\\nForthebasemodels,weusedasinglemodelobtainedbyaveragingthelast5checkpoints,which\\nwerewrittenat10-minuteintervals. Forthebigmodels,weaveragedthelast20checkpoints. We\\nusedbeamsearchwithabeamsizeof4andlengthpenaltyα = 0.6[38]. Thesehyperparameters',\n",
              " 'usedbeamsearchwithabeamsizeof4andlengthpenaltyα = 0.6[38]. Thesehyperparameters\\nwerechosenafterexperimentationonthedevelopmentset. Wesetthemaximumoutputlengthduring\\ninferencetoinputlength+50,butterminateearlywhenpossible[38].\\nTable2summarizesourresultsandcomparesourtranslationqualityandtrainingcoststoothermodel\\narchitecturesfromtheliterature. Weestimatethenumberoffloatingpointoperationsusedtotraina\\nmodelbymultiplyingthetrainingtime,thenumberofGPUsused,andanestimateofthesustained',\n",
              " 'modelbymultiplyingthetrainingtime,thenumberofGPUsused,andanestimateofthesustained\\nsingle-precisionfloating-pointcapacityofeachGPU5.\\n6.2 ModelVariations\\nToevaluatetheimportanceofdifferentcomponentsoftheTransformer,wevariedourbasemodel\\nindifferentways,measuringthechangeinperformanceonEnglish-to-Germantranslationonthe\\n5Weusedvaluesof2.8,3.7,6.0and9.5TFLOPSforK80,K40,M40andP100,respectively.\\n8\\nTable3: VariationsontheTransformerarchitecture. Unlistedvaluesareidenticaltothoseofthebase',\n",
              " '8\\nTable3: VariationsontheTransformerarchitecture. Unlistedvaluesareidenticaltothoseofthebase\\nmodel. AllmetricsareontheEnglish-to-Germantranslationdevelopmentset,newstest2013. Listed\\nperplexitiesareper-wordpiece,accordingtoourbyte-pairencoding,andshouldnotbecomparedto\\nper-wordperplexities.\\ntrain PPL BLEU params\\nN d d h d d P ϵ\\nmodel ff k v drop ls steps (dev) (dev) ×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n(A)\\n16 32 32 4.91 25.8',\n",
              " '1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n(A)\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n16 5.16 25.1 58\\n(B)\\n32 5.01 25.4 60\\n2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n(C) 256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n0.0 5.77 24.6\\n0.2 4.95 25.5\\n(D)\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positionalembeddinginsteadofsinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\ndevelopmentset,newstest2013. Weusedbeamsearchasdescribedintheprevioussection,butno',\n",
              " 'developmentset,newstest2013. Weusedbeamsearchasdescribedintheprevioussection,butno\\ncheckpointaveraging. WepresenttheseresultsinTable3.\\nInTable3rows(A),wevarythenumberofattentionheadsandtheattentionkeyandvaluedimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattentionis0.9BLEUworsethanthebestsetting,qualityalsodropsoffwithtoomanyheads.\\nInTable3rows(B),weobservethatreducingtheattentionkeysized hurtsmodelquality. This\\nk',\n",
              " 'InTable3rows(B),weobservethatreducingtheattentionkeysized hurtsmodelquality. This\\nk\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunctionthandotproductmaybebeneficial. Wefurtherobserveinrows(C)and(D)that,asexpected,\\nbiggermodelsarebetter,anddropoutisveryhelpfulinavoidingover-fitting.Inrow(E)wereplaceour\\nsinusoidalpositionalencodingwithlearnedpositionalembeddings[9],andobservenearlyidentical\\nresultstothebasemodel.\\n6.3 EnglishConstituencyParsing',\n",
              " 'resultstothebasemodel.\\n6.3 EnglishConstituencyParsing\\nToevaluateiftheTransformercangeneralizetoothertasksweperformedexperimentsonEnglish\\nconstituencyparsing. Thistaskpresentsspecificchallenges: theoutputissubjecttostrongstructural\\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\\nmodelshavenotbeenabletoattainstate-of-the-artresultsinsmall-dataregimes[37].\\nWetraineda4-layertransformerwithd =1024ontheWallStreetJournal(WSJ)portionofthe\\nmodel',\n",
              " 'Wetraineda4-layertransformerwithd =1024ontheWallStreetJournal(WSJ)portionofthe\\nmodel\\nPennTreebank[25],about40Ktrainingsentences. Wealsotraineditinasemi-supervisedsetting,\\nusingthelargerhigh-confidenceandBerkleyParsercorporafromwithapproximately17Msentences\\n[37]. Weusedavocabularyof16KtokensfortheWSJonlysettingandavocabularyof32Ktokens\\nforthesemi-supervisedsetting.\\nWeperformedonlyasmallnumberofexperimentstoselectthedropout,bothattentionandresidual',\n",
              " 'Weperformedonlyasmallnumberofexperimentstoselectthedropout,bothattentionandresidual\\n(section5.4),learningratesandbeamsizeontheSection22developmentset,allotherparameters\\nremained unchanged from the English-to-German base translation model. During inference, we\\n9\\nTable4: TheTransformergeneralizeswelltoEnglishconstituencyparsing(ResultsareonSection23\\nofWSJ)\\nParser Training WSJ23F1\\nVinyals&Kaiserelal. (2014)[37] WSJonly,discriminative 88.3\\nPetrovetal. (2006)[29] WSJonly,discriminative 90.4',\n",
              " 'Petrovetal. (2006)[29] WSJonly,discriminative 90.4\\nZhuetal. (2013)[40] WSJonly,discriminative 90.4\\nDyeretal. (2016)[8] WSJonly,discriminative 91.7\\nTransformer(4layers) WSJonly,discriminative 91.3\\nZhuetal. (2013)[40] semi-supervised 91.3\\nHuang&Harper(2009)[14] semi-supervised 91.3\\nMcCloskyetal. (2006)[26] semi-supervised 92.1\\nVinyals&Kaiserelal. (2014)[37] semi-supervised 92.1\\nTransformer(4layers) semi-supervised 92.7\\nLuongetal. (2015)[23] multi-task 93.0\\nDyeretal. (2016)[8] generative 93.3',\n",
              " 'Luongetal. (2015)[23] multi-task 93.0\\nDyeretal. (2016)[8] generative 93.3\\nincreasedthemaximumoutputlengthtoinputlength+300. Weusedabeamsizeof21andα=0.3\\nforbothWSJonlyandthesemi-supervisedsetting.\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\\nprisinglywell,yieldingbetterresultsthanallpreviouslyreportedmodelswiththeexceptionofthe\\nRecurrentNeuralNetworkGrammar[8].\\nIncontrasttoRNNsequence-to-sequencemodels[37],theTransformeroutperformstheBerkeley-',\n",
              " 'IncontrasttoRNNsequence-to-sequencemodels[37],theTransformeroutperformstheBerkeley-\\nParser[29]evenwhentrainingonlyontheWSJtrainingsetof40Ksentences.\\n7 Conclusion\\nInthiswork,wepresentedtheTransformer,thefirstsequencetransductionmodelbasedentirelyon\\nattention,replacingtherecurrentlayersmostcommonlyusedinencoder-decoderarchitectureswith\\nmulti-headedself-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based',\n",
              " 'For translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-Frenchtranslationtasks,weachieveanewstateoftheart. Intheformertaskourbest\\nmodeloutperformsevenallpreviouslyreportedensembles.\\nWeareexcitedaboutthefutureofattention-basedmodelsandplantoapplythemtoothertasks. We\\nplantoextendtheTransformertoproblemsinvolvinginputandoutputmodalitiesotherthantextand',\n",
              " 'plantoextendtheTransformertoproblemsinvolvinginputandoutputmodalitiesotherthantextand\\ntoinvestigatelocal,restrictedattentionmechanismstoefficientlyhandlelargeinputsandoutputs\\nsuchasimages,audioandvideo. Makinggenerationlesssequentialisanotherresearchgoalsofours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor.\\nAcknowledgements WearegratefultoNalKalchbrennerandStephanGouwsfortheirfruitful\\ncomments,correctionsandinspiration.',\n",
              " 'comments,correctionsandinspiration.\\nReferences\\n[1] JimmyLeiBa,JamieRyanKiros,andGeoffreyEHinton. Layernormalization. arXivpreprint\\narXiv:1607.06450,2016.\\n[2] DzmitryBahdanau,KyunghyunCho,andYoshuaBengio. Neuralmachinetranslationbyjointly\\nlearningtoalignandtranslate. CoRR,abs/1409.0473,2014.\\n[3] DennyBritz,AnnaGoldie,Minh-ThangLuong,andQuocV.Le. Massiveexplorationofneural\\nmachinetranslationarchitectures. CoRR,abs/1703.03906,2017.',\n",
              " 'machinetranslationarchitectures. CoRR,abs/1703.03906,2017.\\n[4] JianpengCheng,LiDong,andMirellaLapata. Longshort-termmemory-networksformachine\\nreading. arXivpreprintarXiv:1601.06733,2016.\\n10\\n[5] KyunghyunCho,BartvanMerrienboer,CaglarGulcehre,FethiBougares,HolgerSchwenk,\\nandYoshuaBengio. Learningphraserepresentationsusingrnnencoder-decoderforstatistical\\nmachinetranslation. CoRR,abs/1406.1078,2014.\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv',\n",
              " '[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprintarXiv:1610.02357,2016.\\n[7] JunyoungChung,ÇaglarGülçehre,KyunghyunCho,andYoshuaBengio. Empiricalevaluation\\nofgatedrecurrentneuralnetworksonsequencemodeling. CoRR,abs/1412.3555,2014.\\n[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\\nnetworkgrammars. InProc.ofNAACL,2016.\\n[9] JonasGehring,MichaelAuli,DavidGrangier,DenisYarats,andYannN.Dauphin. Convolu-',\n",
              " '[9] JonasGehring,MichaelAuli,DavidGrangier,DenisYarats,andYannN.Dauphin. Convolu-\\ntionalsequencetosequencelearning. arXivpreprintarXiv:1705.03122v2,2017.\\n[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850,2013.\\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition,pages770–778,2016.',\n",
              " 'Recognition,pages770–778,2016.\\n[12] SeppHochreiter,YoshuaBengio,PaoloFrasconi,andJürgenSchmidhuber. Gradientflowin\\nrecurrentnets: thedifficultyoflearninglong-termdependencies,2001.\\n[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\\n9(8):1735–1780,1997.\\n[14] ZhongqiangHuangandMaryHarper. Self-trainingPCFGgrammarswithlatentannotations\\nacrosslanguages. InProceedingsofthe2009ConferenceonEmpiricalMethodsinNatural\\nLanguageProcessing,pages832–841.ACL,August2009.',\n",
              " 'LanguageProcessing,pages832–841.ACL,August2009.\\n[15] RafalJozefowicz,OriolVinyals,MikeSchuster,NoamShazeer,andYonghuiWu. Exploring\\nthelimitsoflanguagemodeling. arXivpreprintarXiv:1602.02410,2016.\\n[16] ŁukaszKaiserandSamyBengio. Canactivememoryreplaceattention? InAdvancesinNeural\\nInformationProcessingSystems,(NIPS),2016.\\n[17] ŁukaszKaiserandIlyaSutskever. NeuralGPUslearnalgorithms. InInternationalConference\\nonLearningRepresentations(ICLR),2016.',\n",
              " 'onLearningRepresentations(ICLR),2016.\\n[18] NalKalchbrenner,LasseEspeholt,KarenSimonyan,AaronvandenOord,AlexGraves,andKo-\\nrayKavukcuoglu.Neuralmachinetranslationinlineartime.arXivpreprintarXiv:1610.10099v2,\\n2017.\\n[19] YoonKim,CarlDenton,LuongHoang,andAlexanderM.Rush. Structuredattentionnetworks.\\nInInternationalConferenceonLearningRepresentations,2017.\\n[20] DiederikKingmaandJimmyBa. Adam: Amethodforstochasticoptimization. InICLR,2015.',\n",
              " '[20] DiederikKingmaandJimmyBa. Adam: Amethodforstochasticoptimization. InICLR,2015.\\n[21] OleksiiKuchaievandBorisGinsburg. FactorizationtricksforLSTMnetworks. arXivpreprint\\narXiv:1703.10722,2017.\\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130,2017.\\n[23] Minh-ThangLuong,QuocV.Le,IlyaSutskever,OriolVinyals,andLukaszKaiser. Multi-task',\n",
              " '[23] Minh-ThangLuong,QuocV.Le,IlyaSutskever,OriolVinyals,andLukaszKaiser. Multi-task\\nsequencetosequencelearning. arXivpreprintarXiv:1511.06114,2015.\\n[24] Minh-ThangLuong,HieuPham,andChristopherDManning. Effectiveapproachestoattention-\\nbasedneuralmachinetranslation. arXivpreprintarXiv:1508.04025,2015.\\n11\\n[25] MitchellPMarcus,MaryAnnMarcinkiewicz,andBeatriceSantorini.Buildingalargeannotated\\ncorpusofenglish: Thepenntreebank. Computationallinguistics,19(2):313–330,1993.',\n",
              " 'corpusofenglish: Thepenntreebank. Computationallinguistics,19(2):313–330,1993.\\n[26] DavidMcClosky,EugeneCharniak,andMarkJohnson. Effectiveself-trainingforparsing. In\\nProceedingsoftheHumanLanguageTechnologyConferenceoftheNAACL,MainConference,\\npages152–159.ACL,June2006.\\n[27] AnkurParikh,OscarTäckström,DipanjanDas,andJakobUszkoreit. Adecomposableattention\\nmodel. InEmpiricalMethodsinNaturalLanguageProcessing,2016.\\n[28] RomainPaulus,CaimingXiong,andRichardSocher. Adeepreinforcedmodelforabstractive',\n",
              " '[28] RomainPaulus,CaimingXiong,andRichardSocher. Adeepreinforcedmodelforabstractive\\nsummarization. arXivpreprintarXiv:1705.04304,2017.\\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputationalLinguisticsand44thAnnualMeetingoftheACL,pages433–440.ACL,July\\n2006.\\n[30] OfirPressandLiorWolf. Usingtheoutputembeddingtoimprovelanguagemodels. arXiv',\n",
              " '2006.\\n[30] OfirPressandLiorWolf. Usingtheoutputembeddingtoimprovelanguagemodels. arXiv\\npreprintarXiv:1608.05859,2016.\\n[31] RicoSennrich,BarryHaddow,andAlexandraBirch. Neuralmachinetranslationofrarewords\\nwithsubwordunits. arXivpreprintarXiv:1508.07909,2015.\\n[32] NoamShazeer,AzaliaMirhoseini,KrzysztofMaziarz,AndyDavis,QuocLe,GeoffreyHinton,\\nandJeffDean. Outrageouslylargeneuralnetworks: Thesparsely-gatedmixture-of-experts\\nlayer. arXivpreprintarXiv:1701.06538,2017.',\n",
              " 'layer. arXivpreprintarXiv:1701.06538,2017.\\n[33] NitishSrivastava,GeoffreyEHinton,AlexKrizhevsky,IlyaSutskever,andRuslanSalakhutdi-\\nnov. Dropout: asimplewaytopreventneuralnetworksfromoverfitting. JournalofMachine\\nLearningResearch,15(1):1929–1958,2014.\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. InC.Cortes, N.D.Lawrence, D.D.Lee, M.Sugiyama, andR.Garnett, editors,\\nAdvancesinNeuralInformationProcessingSystems28,pages2440–2448.CurranAssociates,',\n",
              " 'AdvancesinNeuralInformationProcessingSystems28,pages2440–2448.CurranAssociates,\\nInc.,2015.\\n[35] IlyaSutskever,OriolVinyals,andQuocVVLe. Sequencetosequencelearningwithneural\\nnetworks. InAdvancesinNeuralInformationProcessingSystems,pages3104–3112,2014.\\n[36] ChristianSzegedy,VincentVanhoucke,SergeyIoffe,JonathonShlens,andZbigniewWojna.\\nRethinkingtheinceptionarchitectureforcomputervision. CoRR,abs/1512.00567,2015.\\n[37] Vinyals&Kaiser, Koo, Petrov, Sutskever, andHinton. Grammarasaforeignlanguage. In',\n",
              " '[37] Vinyals&Kaiser, Koo, Petrov, Sutskever, andHinton. Grammarasaforeignlanguage. In\\nAdvancesinNeuralInformationProcessingSystems,2015.\\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey,MaximKrikun,YuanCao,QinGao,KlausMacherey,etal. Google’sneuralmachine\\ntranslationsystem: Bridgingthegapbetweenhumanandmachinetranslation. arXivpreprint\\narXiv:1609.08144,2016.\\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with',\n",
              " '[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forwardconnectionsforneuralmachinetranslation. CoRR,abs/1606.04199,2016.\\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\\nshift-reduceconstituentparsing. InProceedingsofthe51stAnnualMeetingoftheACL(Volume\\n1: LongPapers),pages434–443.ACL,August2013.\\n12\\nInput-Input Layer5\\nAttentionVisualizations\\ntI\\ntI\\nsi\\nsi\\nni\\nni\\nsiht\\nsiht\\ntirips\\ntirips\\ntaht\\ntaht\\na\\na\\nytirojam\\nytirojam',\n",
              " 'AttentionVisualizations\\ntI\\ntI\\nsi\\nsi\\nni\\nni\\nsiht\\nsiht\\ntirips\\ntirips\\ntaht\\ntaht\\na\\na\\nytirojam\\nytirojam\\nfo\\nfo\\nnaciremA\\nnaciremA\\nstnemnrevog\\nstnemnrevog\\nevah\\nevah\\ndessap\\ndessap\\nwen\\nwen\\nswal\\nswal\\necnis\\necnis\\n9002\\n9002\\ngnikam\\ngnikam\\neht\\neht\\nnoitartsiger\\nnoitartsiger\\nro\\nro\\ngnitov\\ngnitov\\nssecorp\\nssecorp\\nerom\\nerom\\ntluciffid\\ntluciffid\\n.\\n.\\n>SOE<\\n>SOE<\\n>dap<\\n>dap<\\n>dap<\\n>dap<\\n>dap<\\n>dap<\\n>dap<\\n>dap<\\n>dap<\\n>dap<\\n>dap<\\n>dap<',\n",
              " 'tluciffid\\n.\\n.\\n>SOE<\\n>SOE<\\n>dap<\\n>dap<\\n>dap<\\n>dap<\\n>dap<\\n>dap<\\n>dap<\\n>dap<\\n>dap<\\n>dap<\\n>dap<\\n>dap<\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoderself-attentioninlayer5of6. Manyoftheattentionheadsattendtoadistantdependencyof\\ntheverb‘making’,completingthephrase‘making...moredifficult’. Attentionshereshownonlyfor\\ntheword‘making’. Differentcolorsrepresentdifferentheads. Bestviewedincolor.\\n13\\nInput-Input Layer5\\nehT\\nehT\\nwaL\\nwaL\\nlliw\\nlliw\\nreven\\nreven',\n",
              " '13\\nInput-Input Layer5\\nehT\\nehT\\nwaL\\nwaL\\nlliw\\nlliw\\nreven\\nreven\\neb\\neb\\ntcefrep\\ntcefrep\\n,\\n,\\ntub\\ntub\\nsti\\nsti\\nnoitacilppa\\nnoitacilppa\\ndluohs\\ndluohs\\neb\\neb\\ntsuj\\ntsuj\\n-\\n-\\nsiht\\nsiht\\nsi\\nsi\\ntahw\\ntahw\\new\\new\\nera\\nera\\ngnissim\\ngnissim\\n,\\n,\\nni\\nni\\nym\\nym\\nnoinipo\\nnoinipo\\n.\\n.\\n>SOE<\\n>SOE<\\n>dap<\\n>dap<\\nInput-Input Layer5\\nehT\\nehT\\nwaL\\nwaL\\nlliw\\nlliw\\nreven\\nreven\\neb\\neb\\ntcefrep\\ntcefrep\\n,\\n,\\ntub\\ntub\\nsti\\nsti\\nnoitacilppa\\nnoitacilppa\\ndluohs\\ndluohs\\neb\\neb\\ntsuj\\ntsuj\\n-\\n-\\nsiht\\nsiht\\nsi\\nsi\\ntahw\\ntahw\\new\\new\\nera\\nera\\ngnissim\\ngnissim\\n,\\n,\\nni\\nni',\n",
              " 'dluohs\\ndluohs\\neb\\neb\\ntsuj\\ntsuj\\n-\\n-\\nsiht\\nsiht\\nsi\\nsi\\ntahw\\ntahw\\new\\new\\nera\\nera\\ngnissim\\ngnissim\\n,\\n,\\nni\\nni\\nym\\nym\\nnoinipo\\nnoinipo\\n.\\n.\\n>SOE<\\n>SOE<\\n>dap<\\n>dap<\\nFigure4: Twoattentionheads,alsoinlayer5of6,apparentlyinvolvedinanaphoraresolution. Top:\\nFullattentionsforhead5. Bottom: Isolatedattentionsfromjusttheword‘its’forattentionheads5\\nand6. Notethattheattentionsareverysharpforthisword.\\n14\\nInput-Input Layer5\\nehT\\nehT\\nwaL\\nwaL\\nlliw\\nlliw\\nreven\\nreven\\neb\\neb\\ntcefrep\\ntcefrep\\n,\\n,\\ntub\\ntub\\nsti\\nsti\\nnoitacilppa',\n",
              " 'ehT\\nehT\\nwaL\\nwaL\\nlliw\\nlliw\\nreven\\nreven\\neb\\neb\\ntcefrep\\ntcefrep\\n,\\n,\\ntub\\ntub\\nsti\\nsti\\nnoitacilppa\\nnoitacilppa\\ndluohs\\ndluohs\\neb\\neb\\ntsuj\\ntsuj\\n-\\n-\\nsiht\\nsiht\\nsi\\nsi\\ntahw\\ntahw\\new\\new\\nera\\nera\\ngnissim\\ngnissim\\n,\\n,\\nni\\nni\\nym\\nym\\nnoinipo\\nnoinipo\\n.\\n.\\n>SOE<\\n>SOE<\\n>dap<\\n>dap<\\nInput-Input Layer5\\nehT\\nehT\\nwaL\\nwaL\\nlliw\\nlliw\\nreven\\nreven\\neb\\neb\\ntcefrep\\ntcefrep\\n,\\n,\\ntub\\ntub\\nsti\\nsti\\nnoitacilppa\\nnoitacilppa\\ndluohs\\ndluohs\\neb\\neb\\ntsuj\\ntsuj\\n-\\n-\\nsiht\\nsiht\\nsi\\nsi\\ntahw\\ntahw\\new\\new\\nera\\nera\\ngnissim\\ngnissim\\n,\\n,\\nni\\nni\\nym\\nym\\nnoinipo\\nnoinipo',\n",
              " 'tsuj\\n-\\n-\\nsiht\\nsiht\\nsi\\nsi\\ntahw\\ntahw\\new\\new\\nera\\nera\\ngnissim\\ngnissim\\n,\\n,\\nni\\nni\\nym\\nym\\nnoinipo\\nnoinipo\\n.\\n.\\n>SOE<\\n>SOE<\\n>dap<\\n>dap<\\nFigure5: Manyoftheattentionheadsexhibitbehaviourthatseemsrelatedtothestructureofthe\\nsentence. Wegivetwosuchexamplesabove,fromtwodifferentheadsfromtheencoderself-attention\\natlayer5of6. Theheadsclearlylearnedtoperformdifferenttasks.\\n15']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Section 4: Embedding the Documents**\n",
        "Now we'll create vector embeddings for each text chunk using Gemini's embedding model."
      ],
      "metadata": {
        "id": "ePBcZEBhxNfm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def embed_and_view(text_chunks):\n",
        "    \"\"\"\n",
        "    Embed document chunks and display their numeric embeddings.\n",
        "\n",
        "    Args:\n",
        "        text_chunks (list): List of text chunks from the document\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Initialize the Gemini embeddings\n",
        "        embedding_model = GoogleGenerativeAIEmbeddings(\n",
        "            model=\"models/text-embedding-004\"  # Specify the Gemini Embedding model\n",
        "        )\n",
        "\n",
        "        print(\"Embedding model initialized successfully\")\n",
        "\n",
        "        # Generate and display embeddings for all chunks\n",
        "        for i, chunk in enumerate(text_chunks):\n",
        "            embedding = embedding_model.embed_query(chunk)\n",
        "            print(f\"Chunk {i} Embedding:\\n{embedding}\\n\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error embedding documents: {e}\")\n",
        "\n",
        "# Example usage\n",
        "sample_chunks = [\"This is the first chunk.\", \"This is the second chunk.\", \"And this is the third chunk.\"]\n",
        "embed_and_view(sample_chunks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7VexkrjkzbgA",
        "outputId": "0a8364e0-bb30-4cbd-99ac-dafc199c8fea"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding model initialized successfully\n",
            "Chunk 0 Embedding:\n",
            "[0.005306204780936241, -0.019982466474175453, -0.05330009013414383, -0.037803467363119125, 0.0438869446516037, 0.012169086374342442, 0.011968716979026794, 0.030833037570118904, -0.015381194651126862, 0.02207416482269764, -0.01051324326545, 0.05356165021657944, 0.05694853141903877, 0.013736017979681492, 0.014268357306718826, -0.00033483054721727967, 0.026143047958612442, 0.002164868637919426, -0.10417395830154419, 0.03183707222342491, 0.0369376577436924, -0.026903631165623665, 0.035999879240989685, -0.041685134172439575, -0.014223109930753708, 0.002302129054442048, 0.00924470741301775, -0.036460429430007935, 0.037307705730199814, 0.0015566367655992508, 0.058599747717380524, 0.05178055167198181, -0.0052936505526304245, -0.04410144314169884, 0.014856294728815556, 0.018107743933796883, -0.0010075304890051484, 0.017477499321103096, 0.024988515302538872, -0.02734184078872204, -0.08513811230659485, 0.0653407871723175, -0.025275081396102905, 0.03685902804136276, -0.0012122276239097118, -0.03170057758688927, 0.023002779111266136, 0.055509164929389954, -0.038971900939941406, 0.053492218255996704, 0.031223395839333534, 0.02774263359606266, -0.08576221019029617, 0.020939450711011887, -0.0073637389577925205, -0.009277047589421272, -0.011915070004761219, -0.0168986264616251, 0.04497373104095459, -0.028253600001335144, 0.02167319506406784, -0.019122842699289322, -0.0028407343197613955, -0.023850182071328163, -0.011924381367862225, -0.02064015530049801, 0.009117959067225456, -0.008452409878373146, -0.05372999981045723, 0.03575281798839569, -0.03389148786664009, 0.03964614495635033, -0.03400065377354622, -0.0018342080293223262, -0.007014031521975994, -0.030676651746034622, -0.03875737637281418, -0.033814769238233566, -0.016682051122188568, 0.025308288633823395, -0.048363108187913895, 0.028138624504208565, 0.08768487721681595, 0.07204877585172653, 0.007539923768490553, 0.016955317929387093, 0.03487224504351616, -0.03974111005663872, -0.053915683180093765, -0.023749595507979393, 0.12134454399347305, 0.011347223073244095, 0.0021639708429574966, -0.008416229858994484, 0.09094967693090439, -0.04516078531742096, -0.11350929737091064, -0.09344266355037689, 0.08447153121232986, 0.019075902178883553, 0.01119966059923172, 0.004514235071837902, -0.013782395981252193, -0.0462179034948349, 0.024416567757725716, 0.05560813844203949, -0.050158172845840454, -0.036470722407102585, 0.004334138706326485, 0.010626009665429592, -0.033061373978853226, -0.027937984094023705, 0.001995344180613756, -0.0031517574097961187, -0.003304325742647052, -0.013746581971645355, -0.027973594143986702, 0.013465691357851028, -0.03809302672743797, 0.004813132807612419, -0.00903028342872858, 0.00900539942085743, -0.007603974547237158, 0.08860976248979568, 0.032358117401599884, -0.03455529361963272, -0.006553541403263807, 0.011548327282071114, -0.05015689507126808, -0.044292349368333817, 0.05385381355881691, -0.011135667562484741, 0.03306126594543457, 0.0406167209148407, -0.02924594096839428, -0.033547982573509216, 0.05730659142136574, -0.01718110777437687, 0.044746946543455124, 0.060134407132864, 0.012866009958088398, -0.011316968128085136, -0.03830175846815109, 0.017097407951951027, 0.011938813142478466, 0.010504904203116894, 0.011480629444122314, 0.05823660269379616, -0.05923173949122429, 0.02864876575767994, -0.01756560243666172, -0.025254664942622185, 0.02010767161846161, 0.005956044886261225, -0.013483269140124321, -0.013953384011983871, 0.050755247473716736, -0.046695902943611145, 0.04756278917193413, -0.0048426007851958275, 0.03808775916695595, -0.02567574754357338, -0.013135518878698349, 0.03451511263847351, -0.029200656339526176, -0.0044219475239515305, 0.00015134007844608277, -0.052909307181835175, 0.021631460636854172, 0.02134895697236061, -0.08170599490404129, -0.052718788385391235, -0.061688896268606186, -0.11689472198486328, 0.004225230775773525, 0.00395979592576623, -0.01104870904237032, 0.003464381443336606, -0.04135877639055252, 0.010513359680771828, 0.11655616015195847, 0.02785443887114525, -0.044490914791822433, -0.05447739362716675, 0.04987417533993721, -0.01798817701637745, 0.019868383184075356, 0.06407033652067184, 0.0853017047047615, 0.053476568311452866, -0.04979488253593445, 0.04765475168824196, -0.034698426723480225, 0.05566523224115372, -0.0334586538374424, 0.036193445324897766, 0.013228124938905239, -0.048819735646247864, -0.021783247590065002, -0.01709878258407116, 0.04643937572836876, 0.008450094610452652, -0.029548048973083496, -0.04414265602827072, -0.020705126225948334, 0.03597543016076088, -0.0427926704287529, -0.07045956701040268, -0.001351001556031406, 0.0017948418390005827, 0.005554628558456898, -0.01856134459376335, 0.0028002490289509296, -0.06106742098927498, 0.07109292596578598, 0.016409799456596375, 0.07876352220773697, 0.02201751619577408, 0.03331286832690239, -0.008104074746370316, 0.04084383696317673, 0.02662157639861107, 0.05674617365002632, 0.011306474916636944, 0.005024741869419813, -0.018175750970840454, -0.058237165212631226, -0.009512785822153091, 0.02347603626549244, -0.034862034022808075, 0.016230717301368713, 0.020251473411917686, 0.017426369711756706, 0.06705738604068756, -0.09073900431394577, 0.054261401295661926, 0.022061584517359734, -0.023712599650025368, -0.044641345739364624, 0.012645737268030643, 0.00643452862277627, 0.03637504205107689, 0.04196380823850632, 0.032680071890354156, 0.02108107879757881, -0.0369095541536808, 0.06641369313001633, 0.05589586868882179, -0.03729145973920822, -0.03646427020430565, -0.025248834863305092, -0.019033586606383324, 0.00801872182637453, -0.0065992665477097034, -0.05340274050831795, -0.034896619617938995, 0.03982508182525635, -0.007250503171235323, 0.008520965464413166, -0.06825779378414154, 0.07372225821018219, 0.01086515374481678, -0.017257893458008766, -0.1164744645357132, -0.053410954773426056, -0.07071651518344879, 0.003110087476670742, -0.00441600801423192, 0.035337384790182114, -0.04925085976719856, -0.001179870800115168, -0.0065527912229299545, -0.03907988220453262, -0.023002928122878075, -0.02861187234520912, 0.031042367219924927, 0.005339006427675486, 0.010078161023557186, -0.01447205152362585, -0.067701555788517, -0.006107590161263943, -0.007429767865687609, -0.0294609647244215, 0.010698896832764149, 0.041878849267959595, -0.05476616695523262, -0.01913035847246647, 0.03669547662138939, -0.014451321214437485, -0.009236683137714863, 0.06306129693984985, 0.061471085995435715, -0.010044186376035213, -0.04985719546675682, 0.04904918745160103, 0.023831572383642197, 0.005045609083026648, 0.01598871313035488, -0.019550377503037453, -0.038342930376529694, 0.031217370182275772, 0.052023496478796005, -0.0021864771842956543, 0.05534520000219345, 0.013141228817403316, -0.024343613535165787, -0.04251242056488991, -0.04629617556929588, 0.020936865359544754, 0.02204406075179577, 0.005313524045050144, 0.040231648832559586, -0.036616358906030655, -0.03865813463926315, -0.010967692360281944, -0.03864354267716408, -0.18601065874099731, -0.03891737014055252, 0.013633165508508682, 0.006360155530273914, 0.02189399115741253, 0.03450983762741089, -0.018185319378972054, 0.006640284322202206, 0.04458772391080856, -0.008710719645023346, -0.007007406558841467, 0.001175903482362628, 0.011688302271068096, 0.006897093262523413, 0.01108867209404707, 0.01967463828623295, -0.06051385775208473, -0.04106219857931137, -0.0032105203717947006, 0.0419420525431633, -0.0184140857309103, 0.00465416768565774, 0.051872000098228455, 0.008425986394286156, 0.052179597318172455, 0.05786041542887688, 0.02862982638180256, 0.036405570805072784, -0.015166905708611012, -0.05414383113384247, -0.022588219493627548, -0.025253891944885254, 0.02190575562417507, 0.047062382102012634, 0.03645005077123642, 0.03893871605396271, 0.025021139532327652, -0.03324205055832863, -0.027366794645786285, 0.016313670203089714, 0.08898896723985672, -0.016552461311221123, 0.0008986868197098374, -0.04232553020119667, -0.036891501396894455, 0.006270714569836855, -0.028080536052584648, 0.009640654549002647, 0.012325362302362919, -0.024559251964092255, 7.112427283573197e-06, 0.06857500970363617, -0.036923084408044815, -0.03814125806093216, -0.0482654944062233, -0.005453452467918396, -0.00503004714846611, -0.002598237246274948, 0.02734128013253212, -0.005804030690342188, -0.052267033606767654, 0.013476883061230183, 0.014850031584501266, -0.005361884366720915, 0.0028491478879004717, 0.014831418171525002, -0.012844560667872429, 0.024167969822883606, -0.0361461415886879, 0.07702501863241196, -0.03220000118017197, 0.0012430113274604082, -0.009804245084524155, -0.041886162012815475, -0.03674636408686638, 0.04117625951766968, 0.01549825631082058, 0.023040685802698135, 0.008605546317994595, 0.06455712020397186, -0.02883317321538925, 0.04568449780344963, 0.031931713223457336, -0.008110723458230495, -0.02724526636302471, -0.005173238459974527, 0.05537678301334381, -0.07419387251138687, -0.0019809119403362274, -0.036939628422260284, 0.09160840511322021, 0.010537848807871342, 0.0055663809180259705, 0.017785528674721718, -0.0008787275291979313, -0.03681692108511925, -0.05428753048181534, 0.023773163557052612, -0.02547461912035942, 0.007753060664981604, -0.014832663349807262, -0.01617654412984848, -0.016380993649363518, 0.02284093201160431, -0.05484827235341072, 0.02725307270884514, -0.0007663783035241067, 0.001670251484028995, -0.007053757086396217, -0.0767923891544342, -0.019221408292651176, 0.023814713582396507, 0.018105074763298035, -0.021590285003185272, 0.0365726463496685, 0.07171439379453659, 0.002913967939093709, 0.07159876823425293, 0.004388673696666956, 0.023417362943291664, -0.0051109278574585915, -0.0022967581171542406, 0.019498256966471672, -0.006106766406446695, 0.019357647746801376, 0.009517591446638107, 0.03581896051764488, 0.021812263876199722, -0.016008349135518074, 0.038451436907052994, 0.039494745433330536, 0.01929655112326145, 0.0010068141855299473, -0.02340209111571312, 0.0007236619130708277, -0.0010057264007627964, 0.009234337136149406, -0.021378517150878906, -0.11131229251623154, 0.013693438842892647, 0.02289455756545067, 0.006602820008993149, -0.015538770705461502, -0.020848099142313004, -0.022917114198207855, -0.023629458621144295, 0.09302877634763718, 0.012472102418541908, 0.011867595836520195, 0.010935783386230469, -0.027665063738822937, 0.05635937675833702, -0.04225536063313484, 0.06681154668331146, 0.029823265969753265, -0.005627871956676245, 0.010595750994980335, 0.015188301913440228, -0.010007441975176334, 0.0009404964512214065, 0.061275262385606766, -0.009276758879423141, -0.009239787235856056, -0.002650152426213026, -0.013544400222599506, 0.001049231388606131, -0.010829631239175797, 0.0016738672275096178, 0.06460852175951004, -0.025231942534446716, -0.040952425450086594, 0.04311339929699898, 0.029995022341609, 0.0079389913007617, -0.027768095955252647, -0.0013250301126390696, -0.04163140058517456, -0.028213700279593468, -0.011584817431867123, 0.021428171545267105, 0.03960288316011429, -0.02439934015274048, 0.003092079656198621, 0.0558725968003273, 0.03537104278802872, 0.02178923226892948, -0.038956377655267715, -0.07784908264875412, -0.08282321691513062, 0.012381418608129025, -0.011573479510843754, -0.015345401130616665, 0.023244114592671394, 0.037110693752765656, 0.07557689398527145, -0.022939054295420647, -0.02518729493021965, 0.030312329530715942, 0.011056541465222836, 0.03569861128926277, -0.058776386082172394, -0.015912629663944244, -0.040877293795347214, 0.028294935822486877, -0.04102352634072304, -0.028381872922182083, 0.03843246027827263, -0.011090935207903385, 0.012928172945976257, -0.010628890246152878, -0.005636387970298529, 0.042529862374067307, 0.03780025988817215, -0.03985021635890007, 0.0005374251632019877, 0.06829479336738586, 0.034615591168403625, 0.009768933989107609, 0.020081207156181335, 0.05680779740214348, 0.018343141302466393, 0.02676711417734623, -0.027659371495246887, 0.01962638460099697, 0.010562211275100708, -0.01390993595123291, -0.017171528190374374, 0.06143581122159958, 0.025711840018630028, 0.0027987402863800526, 0.011728140525519848, 0.05004610866308212, 0.006258642300963402, 0.0712592825293541, -0.053669996559619904, -0.03973279148340225, -0.02715080976486206, -0.012240421958267689, 0.005925947334617376, -0.02642034739255905, 0.01900639757514, -0.04203108698129654, -0.04105181619524956, -0.010432546958327293, -0.0014767105458304286, -0.019111618399620056, 0.008728841319680214, -0.0016944564413279295, 0.0010214752983301878, -0.01557998638600111, 0.026514317840337753, 0.05175630375742912, -0.008859331719577312, 0.010657357051968575, -0.03269488736987114, -0.010762182995676994, 0.016788069158792496, -0.021919138729572296, 0.007612260058522224, 0.0017950973706319928, -0.05314198136329651, 0.01942949742078781, 0.02917172946035862, -0.06273022294044495, 0.02294965833425522, -0.0017931831534951925, 0.03961779549717903, -0.020002933219075203, 0.01886630617082119, 0.009142258204519749, 0.0236600860953331, 0.007058010436594486, 0.008512341417372227, -0.017505448311567307, -0.010223892517387867, 0.047597065567970276, -0.0021424409933388233, -0.016964295879006386, 0.0069539500400424, 0.0013229199685156345, -0.028561053797602654, 0.04590071365237236, 0.021204886958003044, 0.003688476048409939, -0.007247793022543192, -0.004244765266776085, 0.04247706010937691, 0.006521245930343866, -0.026033086702227592, 0.005824441555887461, -0.022336207330226898, -0.026235684752464294, 0.00821849424391985, -0.01309322752058506, -0.003901045536622405, 0.031839899718761444, -0.045386902987957, 0.0065041398629546165, -0.05226576700806618, 0.021215619519352913, -0.03075503371655941, -0.011824212968349457, -0.01146810781210661, 0.014515971764922142, 0.012823686003684998, 0.01018720306456089, 0.005394380539655685, -0.024080820381641388, -0.0717552974820137, 0.01858801208436489, -0.009897488169372082, 0.021016445010900497, 0.05355849489569664, 0.009330575354397297, -0.002600813517346978, 0.04289747029542923, 0.04416628181934357, -0.03723638132214546, -0.02117299847304821, 0.015550177544355392, 0.03380132094025612, 0.03126368671655655, -0.00828692875802517, -0.023478209972381592, 0.02770206518471241, 0.010373672470450401, 0.06277298182249069, 0.023923927918076515, -0.06302976608276367, 0.021046604961156845, 0.031359635293483734, 0.04196256771683693, -0.00477464497089386, 0.04654056206345558, -0.0087180957198143, -0.010502059012651443, -0.009512711316347122, 0.008030079305171967, -0.010527784936130047, 0.05154095217585564, -0.07581020891666412, -0.010776056908071041, -0.0016801151214167476, 0.02988286130130291, 0.04881155118346214, -0.02402944676578045, 0.017342176288366318, -0.0004033452714793384, -0.009405193850398064, -0.012170436792075634, -0.013218251056969166, -0.004102001432329416, -0.0030793296173214912, -0.025972450152039528, 0.01315292064100504, 0.009868660010397434, -0.03755130246281624, 0.012687455862760544, -0.003898578230291605, 0.023930396884679794, 0.03880264237523079, -0.01613669842481613, -0.021792368963360786, 0.010676361620426178, 0.002232765546068549, -0.02021615207195282, 0.0031715540681034327, -0.03611337020993233, 0.012762394733726978, 0.0028895006980746984, 0.04845541715621948, -0.02068997360765934, -0.0330716036260128, 0.03728214651346207, -0.01189759187400341, -0.012735947035253048, -0.05882495269179344, -0.005358836613595486, -0.0302191860973835, 0.03534827008843422, -0.01296447403728962, -0.009293424896895885, -0.04448262229561806, -0.011257431469857693, -0.05953330174088478, 0.019341785460710526, -0.05808475986123085, -0.05100468173623085, 0.017535516992211342, 0.0043765464797616005, 0.027127183973789215, 0.05220693722367287, -0.04405587539076805, 0.01202491857111454, -0.022688737139105797, 0.012357360683381557, 0.0057169790379703045, 0.018862472847104073, 0.0058655766770243645, 0.028613464906811714, 0.05290912836790085, 0.003939474001526833, 0.010665563866496086, 0.012093155644834042, -0.004916238598525524, -0.022053472697734833, 0.031184621155261993, 0.07428234815597534, 0.04272856190800667, 0.01523101981729269, -0.03895019739866257, -0.005280741956084967, -0.039702657610177994, 0.06043088063597679, 0.07144217193126678, -0.04294991120696068, -0.02921161614358425, 0.028303762897849083, -0.02554653398692608, -0.03628956899046898, -0.018242305144667625, -0.01914564147591591, -0.01688688062131405, -0.020972220227122307, 0.03185112774372101, -0.050673309713602066, -0.058298371732234955, -0.005781758110970259, -0.005716719198971987, -0.02404874563217163, -0.006206118036061525, -0.014716538600623608, -0.036432716995477676, -0.01587725803256035, -0.08627506345510483, -0.008174668066203594, 0.012098978273570538, 0.012457000091671944, 0.037878427654504776, -0.012716958299279213, -0.011023148894309998, -0.0025349732022732496, 0.03313148394227028, 0.004669274669140577, -0.023489022627472878, 0.07926863431930542, -0.016149871051311493, 0.030079679563641548, -0.06143718212842941, 0.006958521902561188, 0.02615654282271862, -0.01821131817996502]\n",
            "\n",
            "Chunk 1 Embedding:\n",
            "[0.0031259655952453613, -0.009372086264193058, -0.04919043183326721, -0.030949821695685387, 0.04089296609163284, 0.019287725910544395, 0.030580716207623482, 0.03307301923632622, -0.016563748940825462, 0.02023097313940525, -0.004086560569703579, 0.0604032501578331, 0.05308406800031662, 0.018512418493628502, 0.015960387885570526, -0.0036026109009981155, 0.026833364740014076, 0.010017525404691696, -0.08884723484516144, 0.03222537413239479, 0.03509533777832985, -0.019376037642359734, 0.031327106058597565, -0.03426811099052429, -0.03495090827345848, -0.009167470037937164, -0.010921969078481197, -0.038410767912864685, 0.030655832961201668, -0.00790142547339201, 0.05942906066775322, 0.06014615297317505, -0.0062622190453112125, -0.04222992807626724, 0.012353116646409035, 0.013921850360929966, 0.016864119097590446, 0.004360556602478027, 0.025680426508188248, -0.01910443790256977, -0.09231051057577133, 0.04839683324098587, -0.021740300580859184, 0.04356858506798744, -0.009107175283133984, -0.005367519799619913, 0.02214142680168152, 0.052427519112825394, -0.04024375230073929, 0.0575224794447422, 0.042701493948698044, 0.03240509331226349, -0.07176269590854645, 0.027515055611729622, -0.00837888102978468, -0.012743555940687656, -0.020146695896983147, -0.02508368156850338, 0.031425684690475464, -0.02840445563197136, 0.015698369592428207, -0.003853960894048214, -0.008463581092655659, -0.01998843625187874, -0.003962181508541107, -0.02453755773603916, -0.00013169935846235603, -0.009316669777035713, -0.0632607638835907, 0.0412432998418808, -0.03368769586086273, 0.029983414337038994, -0.05399364233016968, -0.007760635577142239, -0.0004822287301067263, -0.04688568040728569, -0.031066564843058586, -0.03284510597586632, -0.015779267996549606, 0.01529682707041502, -0.045290447771549225, 0.029476627707481384, 0.08414977043867111, 0.07359565049409866, 0.0016893260180950165, 0.01659969426691532, 0.04438960924744606, -0.04174530878663063, -0.05043911933898926, -0.01944204606115818, 0.13020426034927368, 0.01972801238298416, -0.015092277899384499, 0.0018870889907702804, 0.08607235550880432, -0.035819005221128464, -0.10707519203424454, -0.0996522530913353, 0.0826423168182373, 0.018806539475917816, 0.006020737811923027, 0.0026954766362905502, -0.0019438719609752297, -0.03903420642018318, 0.027022404596209526, 0.034636106342077255, -0.04956356808543205, -0.03716221824288368, -0.007680946961045265, 0.004627108573913574, -0.028974929824471474, -0.030899621546268463, 0.001816325937397778, 0.004039685241878033, -0.018093369901180267, -0.020766939967870712, -0.025185424834489822, 0.0010373840341344476, -0.03726501390337944, -0.006557137705385685, -0.0023516567889600992, -0.004999652970582247, -0.014515776187181473, 0.0723346695303917, 0.03125561401247978, -0.018835503607988358, -0.0053875017911195755, 0.0037644917611032724, -0.05916651710867882, -0.04262854903936386, 0.03931674361228943, -0.02493145503103733, 0.04214777797460556, 0.05311241000890732, -0.015801742672920227, -0.046910837292671204, 0.049173858016729355, -0.023625392466783524, 0.04552542045712471, 0.03716322034597397, 0.020648058503866196, -0.014784972183406353, -0.046494681388139725, 0.014081700704991817, -0.00336105702444911, 0.015021691098809242, 0.003847495885565877, 0.053725942969322205, -0.07900676876306534, 0.03664011508226395, -0.03023097664117813, -0.006930310744792223, 0.01879829727113247, 0.009945490397512913, -0.01308357808738947, -0.02283629961311817, 0.04107040539383888, -0.04727065563201904, 0.04204633831977844, -0.014056938700377941, 0.04420136287808418, -0.02702868916094303, -0.02017325907945633, 0.04400555416941643, -0.021390100941061974, -0.019742818549275398, -0.0027987929061055183, -0.04305717349052429, 0.013878827914595604, 0.03485764563083649, -0.07321341335773468, -0.04873072728514671, -0.05781402438879013, -0.124716617166996, -0.00912814773619175, 0.008597992360591888, -0.0015611652052029967, 0.006929326336830854, -0.0354873426258564, 0.01002912875264883, 0.09991059452295303, 0.030517976731061935, -0.04121001437306404, -0.05297965928912163, 0.042896006256341934, -0.021724583581089973, 0.030754245817661285, 0.07139595597982407, 0.07430650293827057, 0.04922385886311531, -0.03984421491622925, 0.04016422852873802, -0.04463827237486839, 0.07624799758195877, -0.03740685433149338, 0.03951573744416237, 0.008559845387935638, -0.03904718905687332, -0.015039114281535149, -0.020799176767468452, 0.04828525707125664, 0.005201653111726046, -0.039545152336359024, -0.041444677859544754, -0.02706361934542656, 0.029494693502783775, -0.04268979653716087, -0.08573947846889496, -0.006230073049664497, 0.004688333719968796, 0.01791241578757763, -0.013601582497358322, 0.0021581952460110188, -0.056083016097545624, 0.061999447643756866, -0.0006412842194549739, 0.08546360582113266, 0.04272250458598137, 0.031147770583629608, -0.012486892752349377, 0.04582185670733452, 0.024731138721108437, 0.048907164484262466, 0.0157974474132061, 0.009171582758426666, -0.028298091143369675, -0.04787692427635193, -0.02393278107047081, 0.03383510559797287, -0.03946519270539284, 0.016378015279769897, 0.009297097101807594, 0.012738061137497425, 0.037700604647397995, -0.10307735949754715, 0.05285130813717842, 0.0004791175597347319, -0.03428482264280319, -0.02908438630402088, 0.00978967547416687, 0.004738281015306711, 0.034605566412210464, 0.056861039251089096, 0.029513230547308922, 0.019502002745866776, -0.028491541743278503, 0.06475488841533661, 0.06965363025665283, -0.019587615504860878, -0.03649670630693436, -0.007630586624145508, -0.02518109232187271, 0.009124687872827053, -0.009111989289522171, -0.052628226578235626, -0.03178299218416214, 0.025396224111318588, 0.001672470010817051, 0.00027959412545897067, -0.06265927851200104, 0.07589063048362732, 0.019748274236917496, -0.006287289783358574, -0.10904856026172638, -0.04127155616879463, -0.07998203486204147, 0.0013121770462021232, -0.0019516771426424384, 0.05552900955080986, -0.033451735973358154, -0.0023432448506355286, -0.009879214689135551, -0.03939235955476761, -0.02363692596554756, -0.04818940907716751, 0.03850710019469261, -0.004559159278869629, 0.005855061579495668, -0.005192991811782122, -0.05851973220705986, -0.009210593067109585, 0.006888941396027803, -0.030016228556632996, 0.003074564738199115, 0.04326743632555008, -0.061692237854003906, -0.010657954961061478, 0.035834524780511856, -0.008028511889278889, -0.01800389029085636, 0.05736079439520836, 0.06252509355545044, -0.019029181450605392, -0.051478415727615356, 0.04633809253573418, 0.01252396497875452, 0.011774108745157719, 0.01693841814994812, -0.03149222582578659, -0.03087467886507511, 0.02891092747449875, 0.03308987244963646, -0.002472775522619486, 0.05309661105275154, 0.009018254466354847, -0.03256811201572418, -0.044368043541908264, -0.043992169201374054, 0.012099241837859154, 0.036171212792396545, 0.009376160800457, 0.04218625649809837, -0.03650781139731407, -0.032209303230047226, -0.022673027589917183, -0.04901669919490814, -0.17537908256053925, -0.028654202818870544, 0.022991184145212173, 0.0011017927899956703, 0.01868271827697754, 0.036509595811367035, -0.03094702586531639, 0.013199602253735065, 0.04914439097046852, 0.006400868762284517, -0.010248911567032337, -0.010503982193768024, 0.018230289220809937, 0.01318341214209795, 0.0019346121698617935, 0.004340020008385181, -0.06597834080457687, -0.05124768987298012, 0.0017338129691779613, 0.04064885154366493, -0.0029431364964693785, 0.0049737077206373215, 0.058395009487867355, 0.0037413600366562605, 0.06316342949867249, 0.06085314601659775, 0.009174269624054432, 0.039662446826696396, -0.019541114568710327, -0.06806866824626923, -0.026114562526345253, -0.019068464636802673, 0.027769921347498894, 0.0484563410282135, 0.03143538534641266, 0.03348260000348091, 0.021073654294013977, -0.040289394557476044, -0.015618051402270794, 0.019914710894227028, 0.0849447250366211, -2.504251096979715e-05, -0.017111491411924362, -0.038892753422260284, -0.04851813241839409, 0.010886335745453835, -0.02044459618628025, 0.003737091552466154, 0.017436284571886063, -0.030290106311440468, -0.001550649874843657, 0.06202726438641548, -0.036736294627189636, -0.03977678343653679, -0.038457538932561874, -0.011233754456043243, -0.004321111366152763, 0.0031060785986483097, 0.03844299912452698, 9.122387564275414e-05, -0.053471025079488754, -0.0010548767168074846, 0.005160038359463215, 0.0011459797387942672, 0.002082482213154435, 0.010209577158093452, -0.00036675206501968205, 0.03414352983236313, -0.04000456631183624, 0.0692935660481453, -0.0351870097219944, -0.01254703663289547, -0.008369795978069305, -0.024541359394788742, -0.03139524906873703, 0.04972384124994278, 0.012714694254100323, 0.029893066734075546, -0.00613054446876049, 0.07536865025758743, -0.023491403087973595, 0.04823722690343857, 0.03625921532511711, -0.009405674412846565, -0.03851896524429321, -0.014855491928756237, 0.05084474757313728, -0.07824424654245377, 0.003675616579130292, -0.032595835626125336, 0.07729756087064743, 0.008573857136070728, -0.0011657843133434653, 0.017151428386569023, -0.008982641622424126, -0.05640358850359917, -0.06173698604106903, 0.009852216579020023, -0.01570412889122963, 0.006874844431877136, -0.009446834214031696, -0.0279286690056324, -0.03225652128458023, 0.025993600487709045, -0.05069107189774513, 0.03872251883149147, 0.006169287953525782, 0.02269092947244644, -0.0011180853471159935, -0.07490773499011993, -0.020164327695965767, 0.02641594409942627, 0.01339365541934967, -0.012441483326256275, 0.03300699591636658, 0.07727028429508209, -0.01185084693133831, 0.06744392961263657, 0.022429903969168663, 0.017141077667474747, -0.004003005102276802, -0.0073669711127877235, 0.006733954884111881, -0.004199756309390068, 0.030268846079707146, 0.014596841298043728, 0.03418821841478348, 0.03083457052707672, -0.008069333620369434, 0.043495677411556244, 0.04829426482319832, 0.01667323149740696, -0.002439664676785469, -0.03227108344435692, 0.004102695267647505, -0.00759925227612257, 0.014080078341066837, -0.026501335203647614, -0.11031873524188995, 0.006371802184730768, 0.02272167056798935, 0.010901976376771927, -0.014691580086946487, -0.024038318544626236, -0.021718688309192657, -0.020111793652176857, 0.0890941247344017, 0.016063537448644638, -0.0012324693379923701, 0.014966441318392754, -0.0034723826684057713, 0.05224697291851044, -0.057332348078489304, 0.060076601803302765, 0.01719043403863907, -0.0052686515264213085, -0.004592907149344683, 0.021753164008259773, -0.0050572301261126995, -0.006144759710878134, 0.043988268822431564, -0.014139670878648758, -0.006848827004432678, -0.009898295626044273, -0.003827647538855672, -0.000995221664197743, -0.014597835950553417, 0.0031193688046187162, 0.05507125332951546, -0.014105756767094135, -0.023172646760940552, 0.03656874969601631, 0.025201594457030296, -0.003156432881951332, -0.021359363570809364, -0.004939562175422907, -0.03954501822590828, -0.02291910909116268, -0.0014439134392887354, 0.023463165387511253, 0.045018360018730164, -0.028175467625260353, -0.0009694535401649773, 0.07921824604272842, 0.036463413387537, 0.022715533152222633, -0.04331095889210701, -0.07719667255878448, -0.07209936529397964, 0.01777811348438263, -0.0030447631143033504, -0.010692335665225983, 0.04272886738181114, 0.02999759092926979, 0.07981095463037491, -0.022000206634402275, -0.03390227258205414, 0.028112303465604782, -0.0015926904743537307, 0.03651425242424011, -0.03518538922071457, -0.014967786148190498, -0.04064923897385597, 0.036722756922245026, -0.04158628359436989, -0.031588394194841385, 0.03912317752838135, -0.020544063299894333, 0.013488338328897953, -0.014790420420467854, 0.0023418511264026165, 0.03660738468170166, 0.042015694081783295, -0.05340098962187767, 0.0064393989741802216, 0.07032296061515808, 0.040711864829063416, 0.004376664292067289, 0.030152922496199608, 0.05476559326052666, 0.015312216244637966, 0.01933889091014862, -0.02502513863146305, 0.02041141502559185, -0.0016304391901940107, -0.024127794429659843, -0.007168868090957403, 0.05219016224145889, 0.03824705630540848, -0.006077502388507128, 0.014838856644928455, 0.04259035363793373, 0.009056241251528263, 0.07271800190210342, -0.05703381821513176, -0.0321776457130909, -0.025908280164003372, -0.018112512305378914, 8.143058221321553e-05, -0.025778314098715782, 0.03614305332303047, -0.032801270484924316, -0.03774721547961235, -0.01577160321176052, -0.0046235257759690285, -0.017949944362044334, -0.000945787294767797, -0.0014560639392584562, -0.009371757507324219, -0.01432896964251995, 0.0200470220297575, 0.05058976262807846, -0.00011583144805626944, -0.0001522900420241058, -0.028826551511883736, -0.020231163129210472, 0.01428605243563652, -0.01831277459859848, 0.008706141263246536, 0.0043849400244653225, -0.04925897344946861, 0.023892929777503014, 0.029253998771309853, -0.05861154943704605, 0.03016325831413269, 0.001159601379185915, 0.03901640325784683, -0.032542772591114044, 0.023760734125971794, 0.0014817819464951754, 0.03298071771860123, -0.00597515981644392, 0.009815947152674198, -0.01493014581501484, -0.024232111871242523, 0.050048891454935074, -0.00598560506477952, -0.01326513197273016, 0.007083952892571688, -0.015366209670901299, -0.033341504633426666, 0.0366685651242733, 0.02419235184788704, 0.015606779605150223, -0.00449770363047719, -0.008599352091550827, 0.05642322450876236, 0.01844552718102932, -0.015365749597549438, -0.009709874168038368, -0.028575709089636803, -0.024536773562431335, 0.002421510871499777, -0.01770867221057415, -0.00990728847682476, 0.023265931755304337, -0.03763977810740471, 0.007706159260123968, -0.028850922361016273, 0.0030384166166186333, -0.023375526070594788, -0.007856771349906921, -0.00024629157269373536, 0.015251989476382732, 0.016435200348496437, 0.011137103661894798, 0.012632074765861034, -0.028948429971933365, -0.08703543990850449, 0.023893369361758232, 0.010687346570193768, 0.023475345224142075, 0.04199904203414917, -0.007879446260631084, -0.00404054531827569, 0.04538062587380409, 0.03961712867021561, -0.03087083250284195, -0.02925094962120056, 0.02220003865659237, 0.021352145820856094, 0.02624037116765976, -0.00431276997551322, -0.01927046850323677, 0.021093305200338364, 0.00674304598942399, 0.06775572150945663, 0.02556104026734829, -0.05466170236468315, 0.03527240827679634, 0.022330792620778084, 0.04736022278666496, -0.0021018602419644594, 0.03012705408036709, -0.018047219142317772, -0.004557977430522442, -0.0016125722322613, 0.0226112250238657, -0.01461032498627901, 0.05115027353167534, -0.07980811595916748, -0.011015297845005989, 0.011584214866161346, 0.02250824309885502, 0.04174500331282616, -0.0049404543824493885, 0.01673768274486065, 0.020556781440973282, -0.009760196320712566, -0.0013696028618142009, -0.01758296601474285, -0.002873676363378763, -0.005720909684896469, -0.021808132529258728, 0.01592518389225006, -0.009319719858467579, -0.029854409396648407, 0.020607752725481987, 0.0027204931247979403, 0.022136617451906204, 0.04260826110839844, -0.01522556971758604, -0.02161899395287037, 0.012586305849254131, 0.0037167733535170555, -0.02372646890580654, -0.001016449648886919, -0.030710747465491295, 0.01923741213977337, -0.00038873610901646316, 0.05006475746631622, -0.014605316333472729, -0.03082038089632988, 0.054161570966243744, -0.00783651415258646, -0.030138608068227768, -0.0592426173388958, -0.018425114452838898, -0.010561549104750156, 0.04447159543633461, -0.01878361962735653, -0.009030009619891644, -0.045605625957250595, 0.0014265953795984387, -0.051448091864585876, 0.019453275948762894, -0.04911556839942932, -0.054138440638780594, 0.02109592594206333, 0.012475644238293171, 0.01505905669182539, 0.04991770535707474, -0.045858774334192276, -0.007927348837256432, -0.01916348934173584, 0.013353794813156128, -0.0011503563728183508, 0.011955100111663342, 0.008725708350539207, 0.026886656880378723, 0.05148514732718468, -0.002798324916511774, 0.011612479574978352, 0.0071925814263522625, -0.0007566079148091376, -0.025845957919955254, 0.03388361260294914, 0.06932997703552246, 0.044249579310417175, 0.021109985187649727, -0.04293554276227951, -0.00404019933193922, -0.0485050193965435, 0.06299620121717453, 0.08392882347106934, -0.03701932728290558, -0.02251407504081726, 0.015291462652385235, -0.010873271152377129, -0.029053717851638794, -0.01015107799321413, -0.016332067549228668, -0.015753529965877533, -0.021916747093200684, 0.02822824753820896, -0.02899322286248207, -0.07255923002958298, -0.020183276385068893, -0.00850622821599245, -0.018267668783664703, -0.012415467761456966, -0.03165539354085922, -0.027965810149908066, -0.01259572897106409, -0.1004798412322998, -0.017305124551057816, 0.004137408919632435, 0.016319282352924347, 0.05223222076892853, -0.028500905260443687, -0.011483301408588886, 0.0023724688217043877, 0.026717979460954666, 0.017861491069197655, -0.03018508106470108, 0.08185023814439774, -0.026368001475930214, 0.025301072746515274, -0.06893488019704819, 0.014608676545321941, 0.03972091153264046, -0.025422848761081696]\n",
            "\n",
            "Chunk 2 Embedding:\n",
            "[0.02456096187233925, -0.021082190796732903, -0.04457347095012665, -0.0216242503374815, 0.03399563580751419, 0.014725285582244396, 0.03249300643801689, 0.03513689339160919, -0.005356453359127045, 0.029671933501958847, -0.012115208432078362, 0.05389121174812317, 0.05430830642580986, 0.02477417141199112, -0.006170712877064943, -0.0027659109327942133, 0.01703548990190029, 0.010879519395530224, -0.0884556993842125, 0.023564115166664124, 0.030156828463077545, -0.01858319528400898, 0.018969379365444183, -0.021998608484864235, -0.03994392976164818, 0.013049530796706676, 0.013134850189089775, -0.045171454548835754, 0.022679366171360016, 0.0038315043784677982, 0.06541787087917328, 0.05117909982800484, -0.010157525539398193, -0.054430652409791946, 0.025473831221461296, 0.013838889077305794, 0.015592330135405064, 0.01646984927356243, 0.04342329874634743, -0.036366384476423264, -0.08408268541097641, 0.04263190180063248, -0.03832056373357773, 0.05968767777085304, -0.0070528616197407246, -0.0077102454379200935, 0.025978362187743187, 0.03831282630562782, -0.04142114520072937, 0.05465279147028923, 0.039634089916944504, 0.03726612403988838, -0.06730397790670395, 0.02424134872853756, -0.012978071346879005, -0.023365726694464684, -0.018947547301650047, -0.010759031400084496, 0.027210956439375877, -0.04025755077600479, -0.0019334027310833335, -0.0076223136857151985, -0.0167359821498394, -0.024186789989471436, -0.007829684764146805, -0.027828237041831017, -0.0010050702840089798, -0.001190698123537004, -0.062390025705099106, 0.034173380583524704, -0.03149326890707016, 0.037497323006391525, -0.04405253008008003, -0.005759560503065586, 0.001039334456436336, -0.03853444755077362, -0.040640708059072495, -0.029129041358828545, -0.00925727840512991, 0.02158096618950367, -0.025529835373163223, 0.029020456597208977, 0.09068671613931656, 0.06497697532176971, 0.007719322107732296, 0.02625729888677597, 0.0445312075316906, -0.051247209310531616, -0.046344660222530365, -0.03283867612481117, 0.1315767765045166, 0.01120799034833908, -0.0022021038457751274, -0.009647426195442677, 0.07251734286546707, -0.03994309529662132, -0.0987531840801239, -0.10231774300336838, 0.08083010464906693, 0.02137082628905773, 0.01211199164390564, 0.0032971862237900496, -0.015148594044148922, -0.06930998712778091, 0.01866811327636242, 0.033508580178022385, -0.03415698930621147, -0.04738733544945717, -0.013730804435908794, 0.005841829814016819, -0.03333944454789162, -0.03229224681854248, -0.0022868411615490913, -0.00516112707555294, -0.004353353288024664, -0.005710036028176546, -0.032676417380571365, 0.0026258090510964394, -0.04933495447039604, -0.02467307634651661, -0.01605517417192459, -0.006529878359287977, -0.006650453899055719, 0.08462508767843246, 0.042394038289785385, -0.02826172485947609, -0.01748792640864849, 0.013194235041737556, -0.036238402128219604, -0.04388975352048874, 0.046783678233623505, -0.03340059518814087, 0.030037160962820053, 0.04813138023018837, -0.0016760439611971378, -0.044570401310920715, 0.04768139123916626, -0.018039561808109283, 0.04876352846622467, 0.04877667501568794, 0.011306727305054665, -0.007774094585329294, -0.05761612951755524, 0.01898340880870819, 0.01275770366191864, 0.026325466111302376, -0.004542823415249586, 0.042292360216379166, -0.07353934645652771, 0.039091773331165314, -0.02568860910832882, -0.014366813004016876, 0.01590120792388916, -0.0008591676014475524, -0.008388756774365902, -0.02503116987645626, 0.0358065702021122, -0.05000833049416542, 0.04677703231573105, -0.013979099690914154, 0.057394567877054214, -0.02242998592555523, -0.012040495872497559, 0.028833972290158272, -0.03325274586677551, -0.015233689919114113, 0.0031782370060682297, -0.04353732243180275, 0.0314398892223835, 0.03484298288822174, -0.07780863344669342, -0.05074746161699295, -0.06340374052524567, -0.1331247240304947, 0.008669178932905197, 0.004467572085559368, -0.01717511937022209, 0.006397854536771774, -0.031233735382556915, -0.013042922131717205, 0.1023925244808197, 0.0296269953250885, -0.04226519539952278, -0.050156138837337494, 0.041730187833309174, -0.02042088843882084, 0.036771684885025024, 0.057229798287153244, 0.08641033619642258, 0.04758056625723839, -0.0254357922822237, 0.03288822993636131, -0.037305187433958054, 0.06673675775527954, -0.04320472106337547, 0.022906972095370293, 0.005285186693072319, -0.03861774876713753, -0.03227091208100319, -0.03395925462245941, 0.046400170773267746, 0.013147296383976936, -0.05566399171948433, -0.04113350436091423, -0.020398350432515144, 0.026490304619073868, -0.054152946919202805, -0.0839574933052063, -0.0056239846162498, 0.01859455369412899, 0.008844687603414059, -0.0002834944170899689, -0.0024402800481766462, -0.037873316556215286, 0.05908384174108505, -0.004503644537180662, 0.0715317577123642, 0.02360113337635994, 0.025467343628406525, -0.02835535630583763, 0.038681935518980026, 0.0315045565366745, 0.01966184563934803, 0.021617302671074867, 0.016455968841910362, -0.030924243852496147, -0.04869077354669571, -0.025469345971941948, 0.03436290845274925, -0.049349211156368256, 0.011209934018552303, 0.022162150591611862, 0.007070277817547321, 0.048828840255737305, -0.09556247293949127, 0.058071114122867584, 0.009729092940688133, -0.04023626819252968, -0.02889142371714115, 0.006447147578001022, 0.011725726537406445, 0.03541180118918419, 0.03803673014044762, 0.024826202541589737, 0.014437112025916576, -0.04141146317124367, 0.06614720821380615, 0.04500950500369072, -0.0032486242707818747, -0.022120965644717216, -0.028434276580810547, -0.02462668903172016, 0.008766666986048222, 0.001917460118420422, -0.056563980877399445, -0.025651514530181885, 0.023656878620386124, -0.008593423292040825, 0.007937281392514706, -0.0558907575905323, 0.06486492604017258, 0.019264131784439087, -0.020141737535595894, -0.11494683474302292, -0.04933791980147362, -0.07060005515813828, -0.012818879447877407, -0.010201667435467243, 0.05582617595791817, -0.017862407490611076, -0.004596585873514414, -0.007573082111775875, -0.045484695583581924, -0.02263198047876358, -0.028966136276721954, 0.034746598452329636, 0.008047611452639103, 0.008888131007552147, -0.002316637896001339, -0.057904575020074844, -0.003890407970175147, 0.0035835099406540394, -0.02174459770321846, 0.0008209333755075932, 0.05446002632379532, -0.05797141045331955, -0.01323382742702961, 0.0438164584338665, 0.00932938139885664, -0.023509548977017403, 0.04163123667240143, 0.05493811145424843, -0.01688571460545063, -0.04792892932891846, 0.041162267327308655, 0.006447761319577694, 0.02162281423807144, 0.028729084879159927, -0.03708064183592796, -0.022843578830361366, 0.02654576115310192, 0.030960122123360634, -0.012170975096523762, 0.05346199497580528, -0.0005089665646664798, -0.028461964800953865, -0.02868109568953514, -0.042994365096092224, 0.0026214318349957466, 0.03419922664761543, -0.001374739222228527, 0.032839469611644745, -0.0417492501437664, -0.04113936424255371, -0.017650986090302467, -0.049921371042728424, -0.17273536324501038, -0.03668144717812538, 0.007417617365717888, -0.00987844169139862, 0.012509589083492756, 0.039422616362571716, -0.028613215312361717, 0.006709557492285967, 0.03840186074376106, -0.01928272470831871, -0.005147115793079138, 0.013193114660680294, 0.013594084419310093, 0.005081554874777794, 0.005490789655596018, 0.0006304834969341755, -0.06785853952169418, -0.051633071154356, -0.0068160719238221645, 0.04444899410009384, -0.014322382397949696, 0.014676538296043873, 0.05580821633338928, 0.012546906247735023, 0.07065211981534958, 0.06620177626609802, 0.013335946947336197, 0.019209792837500572, -0.023405710235238075, -0.07443663477897644, -0.02110762894153595, -0.015251667238771915, 0.017369749024510384, 0.048407118767499924, 0.05396180972456932, 0.043763477355241776, 0.01782805658876896, -0.040345244109630585, -0.01627456210553646, 0.010421506129205227, 0.08752139657735825, -0.008881734684109688, 0.0030472902581095695, -0.0312211811542511, -0.03318995237350464, 0.0006935878191143274, -0.01991971582174301, 0.0048327939584851265, 0.029481714591383934, -0.028255240991711617, 0.006790856830775738, 0.05869494378566742, -0.04128206893801689, -0.04020540788769722, -0.026795925572514534, -0.018923377618193626, -0.009066354483366013, 0.015152093023061752, 0.04022686928510666, -0.0037434017285704613, -0.047328513115644455, 0.00713620288297534, -0.005582333542406559, 0.004158106166869402, 0.0047125909477472305, 0.010090695694088936, -0.0069351643323898315, 0.04744332283735275, -0.04644867777824402, 0.0722079649567604, -0.02638217620551586, -0.009199952706694603, -0.019374126568436623, -0.012602287344634533, -0.03362556919455528, 0.049878500401973724, 0.02363499253988266, 0.03328637778759003, -0.007209479343146086, 0.07748311758041382, -0.024247504770755768, 0.051652371883392334, 0.02549283765256405, -0.008482202887535095, -0.030276022851467133, -0.01116090640425682, 0.051559820771217346, -0.08448461443185806, -0.009382887743413448, -0.04615987464785576, 0.07982669025659561, 0.0011968082981184125, -0.009644192643463612, 0.018894508481025696, 0.006799684371799231, -0.04980550333857536, -0.05836888775229454, 0.01757226139307022, -0.007029050495475531, 0.01671784743666649, -0.0205428097397089, -0.02184242755174637, -0.01060415431857109, 0.027925001457333565, -0.048864904791116714, 0.03148138150572777, 0.010448724962770939, 0.007184956688433886, -0.005967629607766867, -0.06251072883605957, -0.02143929712474346, 0.018291566520929337, 0.007415028754621744, -0.006197260692715645, 0.049099285155534744, 0.07333458214998245, -0.0035733403638005257, 0.06522106379270554, 0.021529650315642357, 0.008535517379641533, -0.020822245627641678, -0.011253459379076958, -0.0024032010696828365, -0.0064348578453063965, 0.01024596020579338, 0.008834523148834705, 0.03218924626708031, 0.03376643359661102, -0.02363111823797226, 0.0394878014922142, 0.04402526840567589, 0.015532225370407104, -0.0009236081386916339, -0.0275249145925045, 0.00865587592124939, -0.02005227468907833, 0.022488852962851524, -0.016596544533967972, -0.10636330395936966, 0.019794346764683723, 0.017309794202446938, 0.0070123509503901005, -0.003759046783670783, -0.023562494665384293, 0.005641501396894455, -0.01596393808722496, 0.08838627487421036, 0.01557336375117302, -0.005157616920769215, 0.0002214982669102028, 0.010168945416808128, 0.06168156489729881, -0.05830860882997513, 0.056396450847387314, 0.017928265035152435, 0.005885862745344639, -0.002165436977520585, 0.02018282748758793, -0.0068384697660803795, -0.0038019721396267414, 0.054917141795158386, -0.010836203582584858, -0.021737193688750267, -0.01251405943185091, -0.0076569062657654285, 0.011601312085986137, -0.02320013754069805, 0.00821180734783411, 0.06304841488599777, -0.008093086071312428, -0.032094769179821014, 0.04143811762332916, 0.032466351985931396, 0.005476864520460367, -0.03227229788899422, -0.013186633586883545, -0.03654157742857933, -0.015309208072721958, -0.007596741896122694, 0.030895957723259926, 0.05305202677845955, -0.031552791595458984, -0.009149864315986633, 0.06499798595905304, 0.024727309122681618, 0.02863464504480362, -0.03584929183125496, -0.07925816625356674, -0.08498626947402954, 0.00833551399409771, 0.006846159230917692, -0.016750145703554153, 0.05039423704147339, 0.01932603493332863, 0.057223670184612274, -0.021855222061276436, -0.030797597020864487, 0.016881849616765976, -0.007560879923403263, 0.03672491014003754, -0.04307473450899124, -0.027509965002536774, -0.033405851572752, 0.042775947600603104, -0.034821830689907074, -0.03752703219652176, 0.030889708548784256, -0.02405538223683834, 0.006448463071137667, -0.008472064509987831, 0.003141009481623769, 0.0345737598836422, 0.04490841180086136, -0.047912828624248505, 0.011139368638396263, 0.07135109603404999, 0.041476693004369736, 0.014123434200882912, 0.02429351396858692, 0.060595061630010605, 0.029774101451039314, 0.03215070441365242, -0.0352078378200531, 0.0084015391767025, 0.0037699085660278797, -0.009359763003885746, -0.0020115503575652838, 0.042995721101760864, 0.04972755163908005, -0.0018304629484191537, 0.010079941712319851, 0.047290824353694916, -0.009617176838219166, 0.054254431277513504, -0.06936907023191452, -0.027771752327680588, -0.014907301403582096, -0.023231426253914833, 0.007228751201182604, -0.03413590043783188, 0.04650674760341644, -0.027298465371131897, -0.021962970495224, -0.023917710408568382, -0.01658894680440426, -0.03901337832212448, -0.003420676104724407, -0.025227870792150497, 0.007679102476686239, -0.015308354049921036, 0.019802015274763107, 0.035064395517110825, -0.015095725655555725, 0.009273670613765717, -0.04029575362801552, -0.020787103101611137, 0.014829444698989391, -0.003007354447618127, 0.010471027344465256, 0.01732519082725048, -0.05241932347416878, 0.01923927292227745, 0.024486051872372627, -0.04678286984562874, 0.014282232150435448, -0.0004033487639389932, 0.03978155925869942, -0.014590341597795486, -6.71783127472736e-05, 0.009856115095317364, 0.011839255690574646, -0.004782354924827814, 0.01321442611515522, -0.008859745226800442, -0.02275766059756279, 0.044827669858932495, -0.013249856419861317, -0.036043331027030945, 0.014174720272421837, -0.023516159504652023, -0.033142685890197754, 0.036459192633628845, 0.00549184950068593, 0.02015606500208378, -0.008129984140396118, -0.007501720916479826, 0.054552022367715836, 0.007513424381613731, -0.032791804522275925, -0.006126591004431248, -0.023874221369624138, -0.038800351321697235, 0.000734761415515095, -0.03983353078365326, -0.004411743488162756, 0.01646401733160019, -0.034976664930582047, 0.006878067273646593, -0.043675608932971954, 0.0001741228043101728, -0.03810346499085426, -0.013691432774066925, -0.00779134314507246, 0.01971525326371193, 0.00644766865298152, 0.009979919530451298, 0.006201498676091433, -0.021551018580794334, -0.0958036407828331, 0.023087749257683754, 0.006517043337225914, 0.018528064712882042, 0.04671207442879677, -0.002275596372783184, -0.011622081510722637, 0.04346727952361107, 0.04014451056718826, -0.03509512543678284, -0.015924466773867607, 0.01622842438519001, 0.03350868821144104, 0.027580253779888153, 0.008722646161913872, -0.008066627196967602, 0.03266376629471779, 0.01339836698025465, 0.053665641695261, 0.005981601309031248, -0.046799369156360626, 0.046478573232889175, 0.024190731346607208, 0.04112298786640167, 0.00786038301885128, 0.0400535948574543, -0.013895818963646889, 0.002161086071282625, -0.009567688219249249, 0.026050129905343056, -0.02520354650914669, 0.05026750639081001, -0.08330728858709335, -0.0009231301955878735, -0.009287591092288494, 0.02644338086247444, 0.03038843534886837, -0.02004358172416687, 0.0062305438332259655, 0.03170224651694298, -0.016118384897708893, 0.002527078380808234, -0.017337946221232414, -0.0033366328570991755, 0.00017064859275706112, -0.006609815172851086, 0.014638092368841171, 0.0016717856051400304, -0.043600939214229584, 0.014713049866259098, -0.006077524274587631, 0.02596104145050049, 0.037729572504758835, -0.01995200850069523, -0.03383871912956238, 0.015376181341707706, 0.006311072502285242, -0.03553248941898346, -0.001200430910103023, -0.026663294062018394, 0.02665066532790661, 2.117307303706184e-05, 0.06000915542244911, -0.026621868833899498, -0.05037543550133705, 0.04521708935499191, -0.011474283412098885, -0.02139548398554325, -0.04824857413768768, -0.012723356485366821, -0.017378490418195724, 0.04301869869232178, -0.030829019844532013, 0.001525838510133326, -0.044904910027980804, 0.005168511997908354, -0.06075667217373848, 0.025285586714744568, -0.07243553549051285, -0.03715386241674423, 0.012070981785655022, 0.002986234612762928, 0.004127213265746832, 0.0467158742249012, -0.035916149616241455, -0.005038856063038111, -0.0254505667835474, 0.0030677488539367914, 0.005261100362986326, 0.01099355798214674, 0.012654398567974567, 0.029864255338907242, 0.0448259674012661, 0.004549914970993996, 0.017131483182311058, 0.004288778640329838, -0.016650432720780373, -0.031004490330815315, 0.032168660312891006, 0.072259321808815, 0.03540685027837753, 0.01852414943277836, -0.04496176168322563, -0.007082690950483084, -0.055951714515686035, 0.06802978366613388, 0.08026119321584702, -0.04408588632941246, -0.019271286204457283, 0.0024715675972402096, -0.02042987570166588, -0.033760011196136475, -0.0016134317265823483, -0.01199356373399496, -0.004935236647725105, -0.024435153231024742, 0.043812695890665054, -0.048260580748319626, -0.08115416020154953, -0.030315840616822243, 0.006300277542322874, -0.006764199584722519, 0.0014850766165181994, -0.018107768148183823, -0.026497507467865944, -0.02850138023495674, -0.08903727680444717, -0.0013731132494285703, 0.013257564976811409, 0.009340468794107437, 0.0569176971912384, -0.011537994258105755, -0.0006421823054552078, -0.00663183955475688, 0.033103764057159424, 0.026750287041068077, -0.03874305635690689, 0.08089599758386612, -0.019373321905732155, 0.018468663096427917, -0.0561818890273571, 0.014694895595312119, 0.046724095940589905, -0.01348025631159544]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def embed_documents(text_chunks):\n",
        "    \"\"\"\n",
        "    Function to generate embeddings for the text chunks.\n",
        "\n",
        "    Args:\n",
        "        text_chunks (list): List of text chunks from the document\n",
        "\n",
        "    Returns:\n",
        "        object: Embedding model for further use\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Initialize the Gemini embeddings\n",
        "        embedding_model = GoogleGenerativeAIEmbeddings(\n",
        "            model=\"models/text-embedding-004\"  # Specify the Gemini Embedding model\n",
        "        )\n",
        "\n",
        "        print(\"Embedding model initialized successfully\")\n",
        "        return embedding_model, text_chunks\n",
        "    except Exception as e:\n",
        "        print(f\"Error embedding documents: {e}\")\n",
        "        return None, None"
      ],
      "metadata": {
        "id": "1o6bZlZ7xJ2q"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedded_documents = embed_documents(text_chunks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wwF693I7xwVh",
        "outputId": "17effc1d-9897-4b06-edac-a7fe8ef9b983"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding model initialized successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Section 5: Storing in Vector Database (ChromaDB)**\n",
        "In this section, we'll store the embedded document chunks in a vector database for efficient semantic search."
      ],
      "metadata": {
        "id": "6VQ1MsRcySuo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def store_embeddings(embedding_model, text_chunks):\n",
        "    \"\"\"\n",
        "    Function to store document embeddings in ChromaDB.\n",
        "\n",
        "    Args:\n",
        "        embedding_model: The embedding model to use\n",
        "        text_chunks (list): List of text chunks to embed and store\n",
        "\n",
        "    Returns:\n",
        "        object: Vector store for retrieval\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Create a vector store from the documents\n",
        "        vectorstore = Chroma.from_texts(\n",
        "            texts=text_chunks,\n",
        "            embedding=embedding_model,\n",
        "            persist_directory=\"./chroma_db\"  # Directory to persist the database\n",
        "        )\n",
        "\n",
        "        # Persist the vector store to disk\n",
        "        vectorstore.persist()\n",
        "\n",
        "        print(f\"Successfully stored {len(text_chunks)} document chunks in ChromaDB\")\n",
        "        return vectorstore\n",
        "    except Exception as e:\n",
        "        print(f\"Error storing embeddings: {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "w8rYUeoK1V5l"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chroma_store = store_embeddings(embedded_documents[0],embedded_documents[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVisy1zJ1ZUC",
        "outputId": "8e910426-1059-403c-a388-60fcb84e4888"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully stored 91 document chunks in ChromaDB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3767196766.py:21: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
            "  vectorstore.persist()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Section 6: Embedding User Queries**\n",
        "When a user submits a query, we need to embed it using the same embedding model to find semantically similar chunks."
      ],
      "metadata": {
        "id": "d4K2EMMv1vhS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def embed_query(query, embedding_model):\n",
        "    \"\"\"\n",
        "    Function to embed the user's query.\n",
        "\n",
        "    Args:\n",
        "        query (str): User's question\n",
        "        embedding_model: The embedding model to use\n",
        "\n",
        "    Returns:\n",
        "        list: Embedded query vector\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Generate embedding for the query\n",
        "        query_embedding = embedding_model.embed_query(query)\n",
        "\n",
        "        print(\"Query embedded successfully\")\n",
        "        return query_embedding\n",
        "    except Exception as e:\n",
        "        print(f\"Error embedding query: {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "4nHUTTOr1oKR"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_query = \"Who are the authors of the Attention paper?\""
      ],
      "metadata": {
        "id": "g0-fEGRs2Nh1"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedded_query = embed_query(user_query, embedded_documents[0])\n",
        "print(embedded_query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hEGGVxY22GqO",
        "outputId": "a32a86b7-dde4-4176-d463-1ba74deff39c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query embedded successfully\n",
            "[0.026780592277646065, 0.015416848473250866, -0.050271518528461456, 0.002554000820964575, -0.0059105996042490005, 0.04259137436747551, 0.026414358988404274, 0.06467478722333908, 0.01493052113801241, 0.0037402184680104256, -0.017083164304494858, 0.013827289454638958, 0.050974294543266296, 0.012638152576982975, -0.004161462187767029, -0.03828955441713333, 0.054527826607227325, 0.016758719459176064, -0.04421818256378174, 0.03707011789083481, -0.0034565047826617956, -0.01614072546362877, -0.0002991380461025983, -0.004321862477809191, 0.00837301928550005, -0.0422188974916935, 0.013816770166158676, -0.04541477933526039, -0.016824742779135704, -0.04866284877061844, -0.0005496578523889184, 0.0390058234333992, 0.0016210851026698947, -0.0188022181391716, 0.01164932269603014, 0.06262259185314178, -0.007935280911624432, 0.028559977188706398, 0.05521153286099434, -0.07550094276666641, -0.035486288368701935, -0.02005232684314251, 0.0053126011043787, 0.06321743130683899, -0.0473003126680851, 0.012340456247329712, 0.0532706156373024, 0.0737450122833252, -0.018383139744400978, 0.06900995224714279, -0.008114011026918888, 0.02295750379562378, -0.07058260589838028, 0.025698063895106316, -0.03681381791830063, -0.025088520720601082, -0.01624574325978756, 0.011486421339213848, 0.07331056892871857, 0.003931027837097645, -0.030572224408388138, -0.006588348187506199, -0.013566234149038792, -0.01856556534767151, 0.0264012198895216, -0.06420198082923889, -0.009450449608266354, 0.010912266559898853, -0.07599235326051712, 0.04640500620007515, 0.020090728998184204, 0.003753597615286708, -0.021602852270007133, 0.042504407465457916, 0.022912850603461266, -0.03574449568986893, -0.022820712998509407, -0.03713354840874672, 0.018606524914503098, 0.05116138607263565, -0.022942515090107918, 0.055344898253679276, 0.10119698196649551, 0.032460086047649384, 0.01709597371518612, 0.01259576715528965, 0.04833747819066048, -0.07697658240795135, -0.04823184385895729, -0.044643618166446686, 0.13075707852840424, -0.007576992269605398, -0.008192765526473522, -0.011384209617972374, 0.06905383616685867, -0.07047323882579803, -0.0732136219739914, -0.051287852227687836, 0.14384959638118744, 0.049267884343862534, -0.0291808620095253, 0.007823888212442398, -0.023024465888738632, -0.05092804878950119, 0.05568244308233261, 0.061606016010046005, 0.027306539937853813, -0.04738708212971687, -0.012360538356006145, 0.02855413593351841, -0.024967310950160027, -0.02502334862947464, -0.01966562122106552, -0.011280855163931847, 0.007995963096618652, -0.02145402692258358, -0.07578279078006744, -0.024045949801802635, -0.03287968412041664, 0.00573630491271615, -0.03936329856514931, 0.051704809069633484, -0.017022427171468735, 0.05806312710046768, 0.03715786337852478, -0.03137136995792389, 0.004282109439373016, -0.036683209240436554, -0.08577015995979309, 0.007772914599627256, 0.08995026350021362, -0.04870429262518883, -0.024927424266934395, -0.012534210458397865, -0.016775023192167282, -0.04603598639369011, 0.053745485842227936, -0.01950480416417122, 0.04648090898990631, 0.05619463324546814, 0.012342668138444424, -0.022713199257850647, -0.026520980522036552, -0.009905979968607426, 0.04199013113975525, -0.009402171708643436, -0.002330620074644685, 0.0721064880490303, 0.036223623901605606, 0.004243149887770414, -0.03124881722033024, 0.004737888928502798, 0.025662075728178024, -0.02592443861067295, -0.042005036026239395, -0.03244224563241005, 0.05599978566169739, -0.04342285543680191, 0.07092991471290588, -0.02094557136297226, 0.022512979805469513, -0.047211721539497375, -0.012007663026452065, 0.016184601932764053, -0.0266422051936388, -0.0005926053272560239, 0.002957416931167245, -0.024261143058538437, 0.001216978533193469, 0.03810223564505577, -0.027005327865481377, -0.028474178165197372, 0.007071364670991898, -0.09530963003635406, 0.0452335961163044, 0.005530380178242922, -0.020353449508547783, -0.01675819419324398, 0.023053131997585297, -0.02350812591612339, 0.07770314067602158, 0.011239203624427319, -0.03532615676522255, -0.05463707819581032, -0.009156148880720139, -0.0314776711165905, 0.05991830676794052, -0.013082996942102909, 0.0604567714035511, 0.017969196662306786, -0.020463498309254646, 0.04010014981031418, 0.03668757528066635, 0.035742420703172684, -0.005966663360595703, -0.04673821106553078, -0.04976222291588783, -0.03287290409207344, 0.01134437695145607, -0.05393847078084946, 0.0057279993779957294, 0.03691776469349861, -0.002301414730027318, -0.04006018489599228, -0.015441853553056717, 0.013693277724087238, -0.03954988718032837, -0.07945957034826279, 0.04960682615637779, 0.02452327497303486, -0.03340572863817215, -0.029050514101982117, -0.014516954310238361, -0.0760694071650505, 0.04158481955528259, -0.01098911464214325, 0.08791583776473999, 0.022452834993600845, 0.05243174731731415, -0.03473956510424614, 4.816893124370836e-05, -0.0019412324763834476, -0.00896339863538742, -0.005540332291275263, 0.020791973918676376, 0.05926075950264931, -0.04420357570052147, -0.011884699575603008, 0.008211714215576649, -0.03984186425805092, 0.012471901252865791, -0.03120802529156208, 0.004553467500954866, 0.03725144639611244, -0.028882691636681557, 0.014635038562119007, 0.001564833684824407, -0.006264373194426298, -0.03828053921461105, 0.007856138981878757, -0.002703513717278838, 0.0805339366197586, 0.014456708915531635, -0.02956491895020008, -0.014655906707048416, 0.03313044458627701, 0.056667692959308624, 0.012029013596475124, -0.021747661754488945, -0.04070303589105606, -0.02186373807489872, -0.004693241789937019, -0.03456636890769005, 0.008176709525287151, -0.08677274733781815, -0.0626419335603714, -0.014111923985183239, -0.016150008887052536, -0.030677640810608864, -0.01837168075144291, 0.019732581451535225, -0.032686084508895874, -0.048866793513298035, -0.0984739437699318, -0.026751473546028137, -0.08627600967884064, -0.004848745185881853, -0.03391280770301819, 0.08433374017477036, 0.0011360491625964642, 0.00398533558472991, -0.009512168355286121, -0.05716599151492119, -0.021351182833313942, -0.03122374415397644, 0.02988995425403118, 0.022251572459936142, -0.0015584593638777733, -0.02076864056289196, 0.00998101755976677, 0.07033542543649673, 0.0466608852148056, -0.04022067412734032, -0.04125408083200455, 0.04903696849942207, -0.02076568268239498, 0.008461684919893742, -0.003768783062696457, -0.03973108530044556, -0.030510753393173218, 0.05427739769220352, 0.0655476525425911, -0.036016251891851425, -0.03284011408686638, 0.010320788249373436, 0.030594564974308014, 0.004974949173629284, 0.04021709784865379, -0.0051900045946240425, 0.03490445017814636, 0.001640934613533318, 0.04657148942351341, -0.030375057831406593, 0.044066932052373886, 0.013597468845546246, -0.03540084511041641, -0.03382130339741707, -0.0221095010638237, -0.08074700087308884, 0.008694843389093876, 0.018599191680550575, 0.014895493164658546, -0.00042948819464072585, -0.03727155551314354, -0.01614360697567463, -0.03268935903906822, -0.16453781723976135, 0.004087803419679403, 0.005712644662708044, -0.01322011649608612, -0.007483603432774544, 0.0030871371272951365, -0.03427030146121979, -0.003590755397453904, 0.03575490042567253, 0.004664047621190548, -0.006654747761785984, 0.012283417396247387, 0.0062689390033483505, 0.0510719008743763, 0.03240395709872246, -0.03116592764854431, -0.029108382761478424, -0.07417450100183487, -0.03749583289027214, 0.007918527349829674, -0.00631525507196784, 0.056746065616607666, 0.08256667852401733, 0.055042803287506104, 0.06838098168373108, 0.016388311982154846, 0.05382874980568886, 0.033542126417160034, -0.009281870909035206, -0.04441278055310249, -0.044152047485113144, 0.012233574874699116, 0.017976872622966766, -0.0037685369607061148, 0.008455761708319187, 0.05282396450638771, -0.0026769174728542566, -0.0581204928457737, -0.011603286489844322, -0.02118518203496933, 0.06568527966737747, -0.020405184477567673, -0.01025240495800972, -0.021600494161248207, 0.0022095772437751293, 0.0177010390907526, 0.014270445331931114, 0.016929803416132927, 0.016406642273068428, -0.027889668941497803, 0.023775355890393257, 0.05631440877914429, -0.010435611940920353, -0.04098571091890335, 0.035927776247262955, 0.0581001378595829, 0.011842888779938221, 0.0003803605795837939, 0.0065369573421776295, -0.0075983088463544846, -0.011389436200261116, 0.06459826231002808, -0.020154256373643875, -0.003268731292337179, -0.012780112214386463, -0.049529626965522766, 0.001705751521512866, 0.017474427819252014, 0.005549536552280188, 0.05865435674786568, -0.058103643357753754, 0.004810509737581015, -0.01651201769709587, -0.012321090325713158, -0.00010480536730028689, 0.07807851582765579, 0.00253009214065969, 0.041645124554634094, -0.001307935337536037, 0.024296727031469345, -0.007977559231221676, 0.029476067051291466, 0.0025482792407274246, 0.016446059569716454, 0.041506413370370865, -0.019098257645964622, 0.07648524641990662, -0.037408508360385895, -0.01061734277755022, 0.043238330632448196, 0.033786848187446594, -0.017419874668121338, 0.016762414947152138, 0.001854729955084622, -0.020039910450577736, -0.01681327074766159, -0.015256412327289581, -0.03372765704989433, -0.03937772288918495, -0.008584648370742798, 0.05497422069311142, -0.01092065591365099, 0.007988931611180305, 0.008795127272605896, -0.00022328170598484576, 0.04401377961039543, 0.048054784536361694, -0.019605198875069618, 0.0023092215415090322, -0.06835264712572098, 0.02218186855316162, -0.019450221210718155, 0.008663340471684933, 0.012857805006206036, 0.0036537188570946455, 0.0564332939684391, -0.02009144239127636, 0.06717390567064285, 0.026377350091934204, 0.05040658637881279, 0.009136746637523174, -0.021385183557868004, 0.002656332915648818, 0.015688885003328323, 0.023750577121973038, -0.018720027059316635, 0.07312121242284775, 0.061712127178907394, -0.0021183930803090334, 0.022675734013319016, 0.031699541956186295, 0.004897181410342455, 0.024827387183904648, -0.05021849274635315, 0.03422077000141144, -0.046584803611040115, -0.014622603543102741, -0.0331377349793911, -0.0750453993678093, -0.04738406836986542, 0.03202557936310768, 0.0121634341776371, -0.045272164046764374, -0.0032923740800470114, -0.007863173261284828, -0.013069617561995983, 0.0407998226583004, 0.034728169441223145, 0.008745349943637848, -0.019409071654081345, 0.035085126757621765, -0.019895318895578384, -0.05272753909230232, 0.04939989000558853, 0.04129662364721298, 0.0016371505334973335, 0.022154102101922035, -0.002167368307709694, -0.09239213168621063, -0.026016270741820335, 0.02688433974981308, 0.03817130997776985, 0.0037264865823090076, -0.04393848404288292, 0.010653134435415268, -0.00011990230996161699, -0.040703702718019485, 0.0093760397285223, 0.034726619720458984, -0.027881531044840813, -0.08990833908319473, 0.0025638167280703783, 0.07847335189580917, -0.01370288711041212, -0.0296217929571867, 0.0070517477579414845, -0.0021090989466756582, -0.00788850337266922, -0.00035286613274365664, 0.023828603327274323, 0.0517793633043766, -0.041434407234191895, 0.0015169403050094843, 0.04946557432413101, 0.07020960748195648, 0.003030229127034545, 0.007648980710655451, -0.05221601203083992, -0.03146497532725334, 0.03646502271294594, -0.03339079022407532, -0.0027552589308470488, 0.020270250737667084, 0.01221823412925005, 0.054511070251464844, -0.019282672554254532, -0.0003124357608612627, -0.007336864713579416, 0.009790363721549511, 0.013261889107525349, -0.03256131336092949, 0.021581418812274933, -0.04177704453468323, -0.0021314083132892847, -0.03585038706660271, 0.02468882128596306, 0.050577256828546524, 0.022198058664798737, 0.04545542970299721, -0.032862916588783264, 0.008095154538750648, -0.018343769013881683, 0.022254033014178276, -0.06105499714612961, -0.009776572696864605, 0.10345116257667542, 0.01739657297730446, 0.01232675090432167, 0.021169129759073257, 0.023059483617544174, 0.007648129481822252, 0.031008554622530937, 0.010552133433520794, 0.011150754056870937, -0.0034833953250199556, 0.026547227054834366, 0.025861836969852448, 0.04160105809569359, 0.06312653422355652, -0.027560152113437653, -0.01892932318150997, 0.05190706253051758, 0.002579797524958849, 0.08197173476219177, -0.012981656938791275, -0.05081194266676903, -0.0005649958038702607, -0.010323498398065567, -0.00962537620216608, -0.025373632088303566, 0.049092940986156464, -0.010328186675906181, -0.0029417758341878653, -0.038663350045681, -0.028283366933465004, -0.04808136820793152, 0.035758256912231445, 0.014538818039000034, -0.014524261467158794, 0.014829304069280624, 0.013362114317715168, 0.025170207023620605, -0.019603202119469643, -0.025363067165017128, 0.02957834117114544, -0.004011248238384724, -0.0014403009554371238, -0.017412250861525536, -0.040395814925432205, -0.02718469686806202, -0.029168372973799706, 0.043463222682476044, 0.0320136696100235, -0.05624275654554367, -0.0029521998949348927, -0.04010940343141556, 0.028374332934617996, 0.006797964219003916, 0.002679841360077262, 0.02101924642920494, -0.0025129413697868586, 0.05295104905962944, 0.011261233128607273, -0.018915973603725433, -0.002969644730910659, 0.04428834840655327, 0.04820197820663452, -0.030898313969373703, 0.011243629269301891, -0.013600409962236881, -0.01156245730817318, -0.0014114947989583015, -0.0022235854994505644, 0.014653871767222881, 0.009277607314288616, -0.004146663937717676, 0.01993231661617756, 0.021338336169719696, 0.010995039716362953, -0.028860604390501976, -0.05171268433332443, -0.03421052545309067, -0.028205392882227898, -0.051019251346588135, -0.02219994179904461, 0.05964183807373047, 0.01712990179657936, 0.018816834315657616, -0.07959503680467606, -0.040834419429302216, -0.018323367461562157, -0.046864625066518784, 0.0052819401025772095, 4.3693158659152687e-05, 0.017708446830511093, 0.04310458153486252, 0.00999949686229229, 0.017814835533499718, 0.0029925669077783823, 0.0693441852927208, -0.019993187859654427, -0.013001842424273491, 0.027853606268763542, -0.009731533005833626, 0.006637520622462034, 0.019915109500288963, 0.02580087073147297, -0.019183123484253883, 0.013713536784052849, 0.019561463966965675, 0.03915199264883995, -0.01485461462289095, -0.020880727097392082, -0.0066595301032066345, 0.05116347223520279, 0.012932992540299892, 0.031789667904376984, 0.04045410454273224, -0.01951780542731285, 0.045030441135168076, 0.006526746787130833, -0.010022182948887348, 0.01594223454594612, 0.01602426916360855, 0.04333584010601044, 0.035065680742263794, -0.03692201152443886, 0.006869039032608271, 0.028051305562257767, 0.0008992801886051893, -0.06558341532945633, 0.00796312466263771, -0.005036556161940098, -0.015431821346282959, 0.039353176951408386, -0.051148612052202225, 0.02158367820084095, 0.015330174937844276, 0.03622535988688469, -0.041371531784534454, -0.007823520340025425, 0.0591290257871151, 0.0486748106777668, -0.020669063553214073, -0.006457222159951925, -0.016985604539513588, 0.005354654975235462, 0.03057292103767395, -0.017753640189766884, 0.03329017013311386, 0.032085735350847244, -0.008650458417832851, -0.010782204568386078, 0.003241610946133733, 0.007017494644969702, -0.05872943624854088, 0.028026049956679344, 0.025881417095661163, 0.03538256138563156, -0.015531424432992935, 0.08517354726791382, 0.029238803312182426, -0.02217722497880459, 0.0381440706551075, -0.04654289782047272, -0.04952205717563629, -0.029526937752962112, 0.012666652910411358, 0.0038928641006350517, 0.026684630662202835, -0.002063511172309518, 0.028364600613713264, -0.04411827400326729, 0.003942421637475491, -0.014620083384215832, -0.009970503859221935, -0.06972669810056686, 0.003538652788847685, -0.012785324826836586, -0.02898869290947914, 0.03698870912194252, -0.0014133908553048968, -0.044466547667980194, -0.005913945846259594, -0.023362159729003906, 0.028025396168231964, 0.045503489673137665, -0.015766851603984833, -0.0074082366190850735, 0.0038871662691235542, 0.033939428627491, -0.019815590232610703, 0.023570343852043152, -0.004835702478885651, -0.011159823276102543, -0.025084402412176132, 0.03370410576462746, 0.048419129103422165, 0.010900761932134628, -0.018551824614405632, -0.05197039246559143, -0.03068535029888153, -0.031014712527394295, 0.0639718547463417, 0.09568702429533005, -0.0021364877466112375, 0.0008556657703593373, 0.036332447081804276, -0.015422467142343521, -0.016814354807138443, -0.021830713376402855, -0.02897605672478676, -0.028839271515607834, -0.03053603135049343, -0.006187557242810726, -0.0010546880075708032, -0.028720123693346977, -0.00038277669227682054, -0.010727651417255402, 0.01839742250740528, -0.024231771007180214, -0.001320939278230071, -0.01386550348252058, -0.01571468822658062, -0.04961651191115379, 0.01804356835782528, 0.03791482746601105, -0.015746645629405975, 0.01477685384452343, -0.017071247100830078, -0.001990597229450941, -0.02371361292898655, 0.014832322485744953, 0.04012478142976761, 0.0028851572424173355, 0.06071889027953148, -0.00993820745497942, 0.027796979993581772, -0.006368484813719988, -0.0042653558775782585, 0.04126206785440445, -0.027636799961328506]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Section 7: Retrieval Process**\n",
        "Now we'll implement the retrieval component that finds the most relevant document chunks based on the user's query."
      ],
      "metadata": {
        "id": "mFlix-cC2nSC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_relevant_chunks(vectorstore, query, embedding_model, k=3):\n",
        "    \"\"\"\n",
        "    Function to retrieve the most relevant document chunks for a query.\n",
        "\n",
        "    Args:\n",
        "        vectorstore: The ChromaDB vector store\n",
        "        query (str): User's question\n",
        "        embedding_model: The embedding model\n",
        "        k (int): Number of chunks to retrieve\n",
        "\n",
        "    Returns:\n",
        "        list: List of relevant document chunks\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Create a retriever from the vector store\n",
        "        retriever = vectorstore.as_retriever(\n",
        "            search_type=\"similarity\",  # Can also use \"mmr\" for Maximum Marginal Relevance\n",
        "            search_kwargs={\"k\": k}     # Number of documents to retrieve\n",
        "        )\n",
        "\n",
        "        # Retrieve relevant chunks\n",
        "        relevant_chunks = retriever.get_relevant_documents(query)\n",
        "\n",
        "        print(f\"Retrieved {len(relevant_chunks)} relevant document chunks\")\n",
        "        return relevant_chunks\n",
        "    except Exception as e:\n",
        "        print(f\"Error retrieving chunks: {e}\")\n",
        "        return []"
      ],
      "metadata": {
        "id": "sQspf4nd2pu8"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "relevant_chunks = retrieve_relevant_chunks(chroma_store, user_query, embedded_documents[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T8XBEeA32zZR",
        "outputId": "c839428a-cb0a-4998-85cd-9f7158546d17"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieved 3 relevant document chunks\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-169283228.py:22: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  relevant_chunks = retriever.get_relevant_documents(query)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "relevant_chunks"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXNicQBA3IAL",
        "outputId": "08997aae-8cba-4f84-aac7-0c0fccc4e146"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={}, page_content='Providedproperattributionisprovided,Googleherebygrantspermissionto\\nreproducethetablesandfiguresinthispapersolelyforuseinjournalisticor\\nscholarlyworks.\\nAttention Is All You Need\\nAshishVaswani∗ NoamShazeer∗ NikiParmar∗ JakobUszkoreit∗\\nGoogleBrain GoogleBrain GoogleResearch GoogleResearch\\navaswani@google.com noam@google.com nikip@google.com usz@google.com\\nLlionJones∗ AidanN.Gomez∗ † ŁukaszKaiser∗\\nGoogleResearch UniversityofToronto GoogleBrain'),\n",
              " Document(metadata={}, page_content='attentionandtheparameter-freepositionrepresentationandbecametheotherpersoninvolvedinnearlyevery\\ndetail.Nikidesigned,implemented,tunedandevaluatedcountlessmodelvariantsinouroriginalcodebaseand\\ntensor2tensor.Llionalsoexperimentedwithnovelmodelvariants,wasresponsibleforourinitialcodebase,and\\nefficientinferenceandvisualizations.LukaszandAidanspentcountlesslongdaysdesigningvariouspartsofand\\nimplementingtensor2tensor,replacingourearliercodebase,greatlyimprovingresultsandmassivelyaccelerating'),\n",
              " Document(metadata={}, page_content='othertasksbyapplyingitsuccessfullytoEnglishconstituencyparsingbothwith\\nlargeandlimitedtrainingdata.\\n∗Equalcontribution.Listingorderisrandom.JakobproposedreplacingRNNswithself-attentionandstarted\\ntheefforttoevaluatethisidea.Ashish,withIllia,designedandimplementedthefirstTransformermodelsand\\nhasbeencruciallyinvolvedineveryaspectofthiswork.Noamproposedscaleddot-productattention,multi-head\\nattentionandtheparameter-freepositionrepresentationandbecametheotherpersoninvolvedinnearlyevery')]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_context_from_chunks(relevant_chunks, splitter=\"\\n\\n---\\n\\n\"):\n",
        "    \"\"\"\n",
        "    Extract page_content from document chunks and join them with a splitter.\n",
        "\n",
        "    Args:\n",
        "        relevant_chunks (list): List of document chunks from retriever\n",
        "        splitter (str): String to use as separator between chunk contents\n",
        "\n",
        "    Returns:\n",
        "        str: Combined context from all chunks\n",
        "    \"\"\"\n",
        "    # Extract page_content from each chunk\n",
        "    chunk_contents = []\n",
        "\n",
        "    for i, chunk in enumerate(relevant_chunks):\n",
        "        if hasattr(chunk, 'page_content'):\n",
        "            # Add a chunk identifier to help with tracing which chunk provided what information\n",
        "            chunk_text = f\"[Chunk {i+1}]: {chunk.page_content}\"\n",
        "            chunk_contents.append(chunk_text)\n",
        "\n",
        "    # Join all contents with the splitter\n",
        "    combined_context = splitter.join(chunk_contents)\n",
        "\n",
        "    return combined_context"
      ],
      "metadata": {
        "id": "pgXPQvpCSrPd"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context = get_context_from_chunks(relevant_chunks)"
      ],
      "metadata": {
        "id": "QZGufvUXSvpc"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "id": "NWjsfIzOjbQu",
        "outputId": "37adc7df-6a82-44be-be85-967b0cdf96ba"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[Chunk 1]: Providedproperattributionisprovided,Googleherebygrantspermissionto\\nreproducethetablesandfiguresinthispapersolelyforuseinjournalisticor\\nscholarlyworks.\\nAttention Is All You Need\\nAshishVaswani∗ NoamShazeer∗ NikiParmar∗ JakobUszkoreit∗\\nGoogleBrain GoogleBrain GoogleResearch GoogleResearch\\navaswani@google.com noam@google.com nikip@google.com usz@google.com\\nLlionJones∗ AidanN.Gomez∗ † ŁukaszKaiser∗\\nGoogleResearch UniversityofToronto GoogleBrain\\n\\n---\\n\\n[Chunk 2]: attentionandtheparameter-freepositionrepresentationandbecametheotherpersoninvolvedinnearlyevery\\ndetail.Nikidesigned,implemented,tunedandevaluatedcountlessmodelvariantsinouroriginalcodebaseand\\ntensor2tensor.Llionalsoexperimentedwithnovelmodelvariants,wasresponsibleforourinitialcodebase,and\\nefficientinferenceandvisualizations.LukaszandAidanspentcountlesslongdaysdesigningvariouspartsofand\\nimplementingtensor2tensor,replacingourearliercodebase,greatlyimprovingresultsandmassivelyaccelerating\\n\\n---\\n\\n[Chunk 3]: othertasksbyapplyingitsuccessfullytoEnglishconstituencyparsingbothwith\\nlargeandlimitedtrainingdata.\\n∗Equalcontribution.Listingorderisrandom.JakobproposedreplacingRNNswithself-attentionandstarted\\ntheefforttoevaluatethisidea.Ashish,withIllia,designedandimplementedthefirstTransformermodelsand\\nhasbeencruciallyinvolvedineveryaspectofthiswork.Noamproposedscaleddot-productattention,multi-head\\nattentionandtheparameter-freepositionrepresentationandbecametheotherpersoninvolvedinnearlyevery'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " final_prompt = f\"\"\"You are a helpful assistant answering questions based on provided context.\n",
        "\n",
        "The context is taken from academic papers, and might have formatting issues like spaces missing between words.\n",
        "Please interpret the content intelligently, separating words properly when they appear joined together.\n",
        "\n",
        "Use ONLY the following context to answer the question.\n",
        "If the answer cannot be determined from the context, respond with \"I cannot answer this based on the provided context.\"\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {user_query}\n",
        "\n",
        "Answer:\"\"\""
      ],
      "metadata": {
        "id": "PhhBceS4TeCq"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "id": "TA3dL2YkUNFr",
        "outputId": "dff586db-3990-45ec-9f70-3373f9f4dc4c"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'You are a helpful assistant answering questions based on provided context.\\n\\nThe context is taken from academic papers, and might have formatting issues like spaces missing between words.\\nPlease interpret the content intelligently, separating words properly when they appear joined together.\\n\\nUse ONLY the following context to answer the question.\\nIf the answer cannot be determined from the context, respond with \"I cannot answer this based on the provided context.\"\\n\\nContext:\\n[Chunk 1]: Providedproperattributionisprovided,Googleherebygrantspermissionto\\nreproducethetablesandfiguresinthispapersolelyforuseinjournalisticor\\nscholarlyworks.\\nAttention Is All You Need\\nAshishVaswani∗ NoamShazeer∗ NikiParmar∗ JakobUszkoreit∗\\nGoogleBrain GoogleBrain GoogleResearch GoogleResearch\\navaswani@google.com noam@google.com nikip@google.com usz@google.com\\nLlionJones∗ AidanN.Gomez∗ † ŁukaszKaiser∗\\nGoogleResearch UniversityofToronto GoogleBrain\\n\\n---\\n\\n[Chunk 2]: attentionandtheparameter-freepositionrepresentationandbecametheotherpersoninvolvedinnearlyevery\\ndetail.Nikidesigned,implemented,tunedandevaluatedcountlessmodelvariantsinouroriginalcodebaseand\\ntensor2tensor.Llionalsoexperimentedwithnovelmodelvariants,wasresponsibleforourinitialcodebase,and\\nefficientinferenceandvisualizations.LukaszandAidanspentcountlesslongdaysdesigningvariouspartsofand\\nimplementingtensor2tensor,replacingourearliercodebase,greatlyimprovingresultsandmassivelyaccelerating\\n\\n---\\n\\n[Chunk 3]: othertasksbyapplyingitsuccessfullytoEnglishconstituencyparsingbothwith\\nlargeandlimitedtrainingdata.\\n∗Equalcontribution.Listingorderisrandom.JakobproposedreplacingRNNswithself-attentionandstarted\\ntheefforttoevaluatethisidea.Ashish,withIllia,designedandimplementedthefirstTransformermodelsand\\nhasbeencruciallyinvolvedineveryaspectofthiswork.Noamproposedscaleddot-productattention,multi-head\\nattentionandtheparameter-freepositionrepresentationandbecametheotherpersoninvolvedinnearlyevery\\n\\nQuestion: Who are the authors of the Attention paper?\\n\\nAnswer:'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response(prompt, model=\"gemini-2.0-flash-thinking-exp-01-21\", temperature=0.3, top_p=0.95):\n",
        "    \"\"\"\n",
        "    Function to generate a response using the Gemini model.\n",
        "\n",
        "    Args:\n",
        "        prompt (str): The prompt for the model\n",
        "\n",
        "    Returns:\n",
        "        str: Model's response\n",
        "    \"\"\"\n",
        "\n",
        "    llm = ChatGoogleGenerativeAI(\n",
        "            model=model,\n",
        "            temperature=0.2,  # Lower temperature for more focused answers\n",
        "            top_p=0.95\n",
        "        )\n",
        "\n",
        "    response = llm.invoke(prompt)\n",
        "\n",
        "    return response.content"
      ],
      "metadata": {
        "id": "92q1mjo7T9tL"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_response(final_prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "RRC_JlaeUwAY",
        "outputId": "c9758f08-05ca-4d94-90a0-6b6937299c39"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The authors of the Attention paper are:\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, and Łukasz Kaiser.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7qTgjNqZMQfZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o4qJniIJMSMM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}